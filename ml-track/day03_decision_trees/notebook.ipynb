{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 03 --- Decision Trees: Theory, Practice, and Comparison\n",
    "\n",
    "Welcome to Day 03 of the ML Track! In Day 02 we built a **logistic regression baseline** and learned\n",
    "how to evaluate classifiers with metrics like accuracy, precision, recall, F1, and ROC-AUC.\n",
    "\n",
    "Today we move to our first **non-linear** model: the **Decision Tree**.\n",
    "\n",
    "## What is a Decision Tree?\n",
    "\n",
    "A decision tree is a flowchart-like model that makes predictions by asking a sequence of yes/no\n",
    "questions about the input features. Think of the childhood game **\"20 Questions\"** --- you narrow\n",
    "down possibilities by asking the most informative question at each step.\n",
    "\n",
    "**Real-world use cases:**\n",
    "\n",
    "| Domain | Example |\n",
    "|---|---|\n",
    "| **Credit scoring** | Is annual income > $50k? Is debt-to-income ratio < 0.3? Approve/deny loan. |\n",
    "| **Medical diagnosis** | Is white blood cell count elevated? Is the patient over 50? Suspect infection vs. other. |\n",
    "| **Customer churn** | Has the customer called support > 3 times? Is their contract month-to-month? Likely to churn. |\n",
    "| **Manufacturing QA** | Is temperature > 200C? Is pressure within tolerance? Flag defective part. |\n",
    "\n",
    "Decision trees are beloved because **you can read them** --- they produce human-interpretable rules.\n",
    "This makes them invaluable in regulated industries (finance, healthcare) where you must **explain**\n",
    "why a model made a particular decision.\n",
    "\n",
    "## What we will cover today\n",
    "\n",
    "1. Theory: how trees split data (Gini impurity, entropy, information gain)\n",
    "2. Theory: bias-variance tradeoff in trees\n",
    "3. Training and evaluating decision trees on the Breast Cancer Wisconsin dataset\n",
    "4. Depth tuning and pruning to control overfitting\n",
    "5. Visualizing and interpreting tree structure\n",
    "6. Feature importance analysis\n",
    "7. Head-to-head comparison with logistic regression from Day 02\n",
    "8. Strengths, weaknesses, and a preview of ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: How Decision Trees Work\n",
    "\n",
    "A decision tree learns by performing **recursive binary splitting**:\n",
    "\n",
    "1. Start with the entire dataset at the **root node**.\n",
    "2. For every feature and every possible threshold, evaluate how well that split separates the classes.\n",
    "3. Choose the (feature, threshold) pair that produces the **purest** child nodes.\n",
    "4. Split the data into two child nodes (left = condition true, right = condition false).\n",
    "5. Repeat steps 2--4 on each child node **recursively**.\n",
    "6. Stop when a **stopping criterion** is met (max depth reached, too few samples, node is already pure).\n",
    "\n",
    "### ASCII Example: A Simple Decision Tree\n",
    "\n",
    "Imagine we are classifying tumors as malignant (M) or benign (B) using two features:\n",
    "\n",
    "```\n",
    "                    [worst radius <= 16.8?]\n",
    "                    /                      \\\n",
    "                 YES                        NO\n",
    "                  |                          |\n",
    "        [mean texture <= 19.5?]         [Predict: MALIGNANT]\n",
    "        /                    \\              (42 samples)\n",
    "     YES                     NO\n",
    "      |                       |\n",
    " [Predict: BENIGN]    [Predict: MALIGNANT]\n",
    "   (310 samples)         (17 samples)\n",
    "```\n",
    "\n",
    "At each **internal node**, the tree asks a question about a single feature.\n",
    "At each **leaf node**, the tree outputs a class prediction (the majority class of samples that ended up there).\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "Without any limits, a tree will keep splitting until every leaf is perfectly pure (contains only\n",
    "one class). This leads to **overfitting**. Common stopping criteria include:\n",
    "\n",
    "- **`max_depth`**: Maximum number of levels from root to any leaf.\n",
    "- **`min_samples_split`**: Minimum samples required to split an internal node.\n",
    "- **`min_samples_leaf`**: Minimum samples required in each leaf.\n",
    "- **`max_leaf_nodes`**: Maximum total number of leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Splitting Criteria --- Gini Impurity and Entropy\n",
    "\n",
    "How does the tree decide which feature and threshold to split on? It uses an **impurity measure**\n",
    "to quantify how mixed the classes are in a node. The goal is to find splits that **reduce impurity\n",
    "the most**.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "The Gini impurity of a node $t$ with $K$ classes is:\n",
    "\n",
    "$$\\text{Gini}(t) = 1 - \\sum_{i=1}^{K} p_i^2$$\n",
    "\n",
    "where $p_i$ is the proportion of samples belonging to class $i$ in node $t$.\n",
    "\n",
    "**Worked example:** A node has 40 malignant and 60 benign samples (100 total).\n",
    "\n",
    "- $p_{\\text{malignant}} = 40/100 = 0.4$\n",
    "- $p_{\\text{benign}} = 60/100 = 0.6$\n",
    "- $\\text{Gini} = 1 - (0.4^2 + 0.6^2) = 1 - (0.16 + 0.36) = 1 - 0.52 = 0.48$\n",
    "\n",
    "A **pure node** (all one class) has Gini = 0. The **maximum impurity** for 2 classes is 0.5 (50/50 split).\n",
    "\n",
    "### Entropy and Information Gain\n",
    "\n",
    "Entropy measures disorder using information theory:\n",
    "\n",
    "$$H(t) = -\\sum_{i=1}^{K} p_i \\log_2(p_i)$$\n",
    "\n",
    "**Worked example** (same node: 40 malignant, 60 benign):\n",
    "\n",
    "- $H = -(0.4 \\times \\log_2 0.4 + 0.6 \\times \\log_2 0.6)$\n",
    "- $H = -(0.4 \\times (-1.322) + 0.6 \\times (-0.737))$\n",
    "- $H = -(-0.529 + (-0.442))$\n",
    "- $H = 0.971$ bits\n",
    "\n",
    "A pure node has entropy = 0. Maximum entropy for 2 classes is 1.0 bit (50/50 split).\n",
    "\n",
    "**Information Gain** is the reduction in entropy after a split:\n",
    "\n",
    "$$\\text{IG} = H(\\text{parent}) - \\left[\\frac{n_{\\text{left}}}{n} H(\\text{left}) + \\frac{n_{\\text{right}}}{n} H(\\text{right})\\right]$$\n",
    "\n",
    "The tree picks the split that **maximizes information gain** (or equivalently, maximizes Gini reduction).\n",
    "\n",
    "### Gini vs Entropy: Comparison\n",
    "\n",
    "| Aspect | Gini Impurity | Entropy |\n",
    "|---|---|---|\n",
    "| **Formula** | $1 - \\sum p_i^2$ | $-\\sum p_i \\log_2 p_i$ |\n",
    "| **Range (2 classes)** | 0 to 0.5 | 0 to 1.0 |\n",
    "| **Computation** | Faster (no logarithm) | Slightly slower |\n",
    "| **Behavior** | Tends to isolate the most frequent class | Tends to produce more balanced trees |\n",
    "| **Default in sklearn** | Yes (criterion='gini') | No (criterion='entropy') |\n",
    "| **When to use** | Good default for most problems | When you want slightly more balanced splits |\n",
    "\n",
    "In practice, the two criteria produce **very similar trees**. Gini is the default because it is\n",
    "marginally faster to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: The Bias-Variance Tradeoff in Decision Trees\n",
    "\n",
    "Decision trees provide a textbook illustration of the **bias-variance tradeoff**:\n",
    "\n",
    "### Deep Trees (Low Bias, High Variance)\n",
    "- A deep tree can model very complex decision boundaries.\n",
    "- It has **low bias** because it can fit almost any pattern in the training data.\n",
    "- But it has **high variance** --- small changes in the training data produce very different trees.\n",
    "- It **memorizes** the training set, including noise. This is **overfitting**.\n",
    "\n",
    "### Shallow Trees (High Bias, Low Variance)\n",
    "- A shallow tree (e.g., depth = 1, called a \"decision stump\") uses very few splits.\n",
    "- It has **high bias** because it cannot capture complex patterns.\n",
    "- But it has **low variance** --- it is stable across different training samples.\n",
    "- It **underfits** because it is too simple for the true pattern. This is **underfitting**.\n",
    "\n",
    "### The Sweet Spot (U-Curve of Test Error)\n",
    "\n",
    "```\n",
    "  Error\n",
    "    |  \\\n",
    "    |   \\   Test Error\n",
    "    |    \\__________\n",
    "    |     Sweet      \\___________\n",
    "    |     Spot!        \\          \\  <-- Overfitting zone\n",
    "    |                   \\\n",
    "    |    ________________\\_________\n",
    "    |   /  Training Error\n",
    "    |  /\n",
    "    +---------------------------------->\n",
    "         Model Complexity (depth)\n",
    "```\n",
    "\n",
    "- As depth increases, **training error** monotonically decreases (eventually to zero).\n",
    "- **Test error** first decreases (model learns real patterns) then increases (model learns noise).\n",
    "- The optimal depth is at the **minimum of the test error curve**.\n",
    "\n",
    "We will see this U-curve clearly in our experiments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Standard library and data manipulation\n",
    "# ============================================================\n",
    "import numpy as np                  # Numerical operations (arrays, math)\n",
    "import pandas as pd                 # DataFrames for tabular data manipulation\n",
    "\n",
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt     # Core plotting library\n",
    "\n",
    "# ============================================================\n",
    "# Scikit-learn: dataset\n",
    "# ============================================================\n",
    "from sklearn.datasets import load_breast_cancer  # Classic binary classification dataset\n",
    "\n",
    "# ============================================================\n",
    "# Scikit-learn: model selection and preprocessing\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split   # Split data into train/test\n",
    "from sklearn.model_selection import cross_val_score     # K-fold cross-validation\n",
    "from sklearn.preprocessing import StandardScaler        # Feature scaling for logistic regression\n",
    "\n",
    "# ============================================================\n",
    "# Scikit-learn: models\n",
    "# ============================================================\n",
    "from sklearn.tree import DecisionTreeClassifier   # The decision tree model\n",
    "from sklearn.tree import plot_tree, export_text    # Visualization utilities for trees\n",
    "from sklearn.linear_model import LogisticRegression  # For comparison with Day 02 baseline\n",
    "\n",
    "# ============================================================\n",
    "# Scikit-learn: evaluation metrics\n",
    "# ============================================================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,          # Overall correct predictions / total predictions\n",
    "    precision_score,         # TP / (TP + FP) -- how many positive predictions were correct\n",
    "    recall_score,            # TP / (TP + FN) -- how many actual positives were found\n",
    "    f1_score,                # Harmonic mean of precision and recall\n",
    "    roc_auc_score,           # Area under ROC curve -- ranking quality\n",
    "    classification_report,   # Full precision/recall/f1 report per class\n",
    "    confusion_matrix,        # 2x2 matrix of TP, TN, FP, FN\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Display settings\n",
    "# ============================================================\n",
    "# Ensure plots appear inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Use a clean plot style for better readability\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set pandas display options so DataFrames render nicely\n",
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"All imports successful. Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the Dataset\n",
    "\n",
    "We use the **Breast Cancer Wisconsin (Diagnostic)** dataset, the same one referenced in Day 02.\n",
    "It contains 569 samples with 30 numeric features computed from digitized images of fine needle\n",
    "aspirates (FNA) of breast masses. The target is binary: **malignant (0)** or **benign (1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from sklearn -- it comes as a Bunch object\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Convert to a pandas DataFrame for easier exploration and manipulation\n",
    "# cancer.data is a 2D numpy array of shape (569, 30)\n",
    "# cancer.feature_names gives us human-readable column names\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# The target variable: 0 = malignant, 1 = benign\n",
    "y = pd.Series(cancer.target, name='target')\n",
    "\n",
    "# Print basic info about the dataset\n",
    "print(f\"Dataset shape: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Target classes: {cancer.target_names}\")\n",
    "print(f\"  0 -> {cancer.target_names[0]} (malignant)\")\n",
    "print(f\"  1 -> {cancer.target_names[1]} (benign)\")\n",
    "print(f\"\\nFeature names:\\n{list(cancer.feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "\n",
    "Before building any model, we should understand our data. For decision trees specifically,\n",
    "we care about:\n",
    "- **Class balance**: Imbalanced classes can bias the tree toward the majority class.\n",
    "- **Feature ranges**: Unlike logistic regression, decision trees do NOT require feature scaling\n",
    "  (they split on thresholds, so the absolute scale does not matter).\n",
    "- **Feature distributions**: Helps us understand what the tree might split on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Class balance ----\n",
    "# Check how many samples belong to each class\n",
    "# A roughly balanced dataset means accuracy is a reasonable metric\n",
    "class_counts = y.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Benign (1):    {class_counts[1]} samples ({class_counts[1]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Malignant (0): {class_counts[0]} samples ({class_counts[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  Ratio (benign/malignant): {class_counts[1]/class_counts[0]:.2f}\")\n",
    "print()\n",
    "\n",
    "# ---- Summary statistics ----\n",
    "# Show min, max, mean, std to understand feature ranges\n",
    "# Notice how features span very different scales (e.g., mean area ~100-2500 vs mean smoothness ~0.05-0.16)\n",
    "# This does NOT matter for decision trees, but DOES matter for logistic regression\n",
    "print(\"Summary statistics (first 10 features):\")\n",
    "X.iloc[:, :10].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize feature ranges by class ----\n",
    "# Plot the means of the first 10 features for each class\n",
    "# This helps us anticipate which features might be strong splitters\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "fig.suptitle('Feature Distributions by Class (first 10 features)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    feature = cancer.feature_names[idx]  # Get the feature name\n",
    "    \n",
    "    # Separate the feature values by class for side-by-side comparison\n",
    "    malignant_vals = X.loc[y == 0, feature]  # Class 0 = malignant\n",
    "    benign_vals = X.loc[y == 1, feature]     # Class 1 = benign\n",
    "    \n",
    "    # Plot overlapping histograms so we can see separation (or lack thereof)\n",
    "    ax.hist(malignant_vals, bins=20, alpha=0.6, color='red', label='Malignant')\n",
    "    ax.hist(benign_vals, bins=20, alpha=0.6, color='steelblue', label='Benign')\n",
    "    ax.set_title(feature, fontsize=9)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features where the two histograms are well-separated (e.g., mean radius, mean concave points)\n",
    "# will likely appear near the top of the tree (high information gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train/Test Split\n",
    "\n",
    "We split the data into training (80%) and test (20%) sets. Key points:\n",
    "- **`stratify=y`** ensures both sets have the same class proportions as the full dataset.\n",
    "  Without stratification, random chance could give us an unrepresentative test set.\n",
    "- **`random_state=42`** makes the split reproducible so results are consistent across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train/test split\n",
    "# test_size=0.2 means 20% of data reserved for testing (~114 samples)\n",
    "# stratify=y preserves the class ratio in both train and test sets\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Verify the split sizes and class proportions\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Benign:    {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  Malignant: {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Test set:  {X_test.shape[0]} samples\")\n",
    "print(f\"  Benign:    {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  Malignant: {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Building a Decision Tree with Scikit-Learn\n",
    "\n",
    "Scikit-learn's `DecisionTreeClassifier` implements the **CART (Classification and Regression Trees)**\n",
    "algorithm. Here are the key parameters you should know:\n",
    "\n",
    "| Parameter | Default | What it controls |\n",
    "|---|---|---|\n",
    "| `criterion` | `'gini'` | Impurity measure: `'gini'` or `'entropy'` |\n",
    "| `max_depth` | `None` | Maximum depth of the tree. `None` = no limit (grow until pure) |\n",
    "| `min_samples_split` | `2` | Minimum samples needed to split an internal node |\n",
    "| `min_samples_leaf` | `1` | Minimum samples in each leaf node |\n",
    "| `max_features` | `None` | Number of features to consider at each split. `None` = all features |\n",
    "| `max_leaf_nodes` | `None` | Maximum number of leaf nodes. `None` = no limit |\n",
    "| `ccp_alpha` | `0.0` | Complexity parameter for cost-complexity pruning (higher = more pruning) |\n",
    "\n",
    "The most important parameters for controlling overfitting are **`max_depth`** and **`ccp_alpha`**.\n",
    "We will experiment with both below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training an Unrestricted Tree (Overfitting Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train a decision tree with NO restrictions ----\n",
    "# max_depth=None means the tree will grow until every leaf is pure\n",
    "# or until min_samples_split is reached (default=2)\n",
    "# This typically leads to a very deep tree that overfits the training data\n",
    "tree_unrestricted = DecisionTreeClassifier(\n",
    "    random_state=42  # Only set random_state for reproducibility; everything else is default\n",
    ")\n",
    "\n",
    "# Fit the tree on training data\n",
    "tree_unrestricted.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on BOTH training and test sets to detect overfitting\n",
    "train_acc_unres = accuracy_score(y_train, tree_unrestricted.predict(X_train))\n",
    "test_acc_unres = accuracy_score(y_test, tree_unrestricted.predict(X_test))\n",
    "\n",
    "print(\"=== Unrestricted Decision Tree ===\")\n",
    "print(f\"Tree depth:      {tree_unrestricted.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_unrestricted.get_n_leaves()}\")\n",
    "print(f\"Training accuracy: {train_acc_unres:.4f}\")\n",
    "print(f\"Test accuracy:     {test_acc_unres:.4f}\")\n",
    "print(f\"Gap (train - test): {train_acc_unres - test_acc_unres:.4f}\")\n",
    "print()\n",
    "print(\"OBSERVATION: Training accuracy is 1.0 (perfect) but test accuracy is lower.\")\n",
    "print(\"This gap is a classic sign of OVERFITTING --- the tree memorized the training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Overfitting in Decision Trees\n",
    "\n",
    "Why do unrestricted decision trees overfit so aggressively?\n",
    "\n",
    "1. **Each leaf can contain as few as 1 sample.** The tree keeps splitting until every training\n",
    "   example is perfectly classified. This means the tree creates hyper-specific rules that\n",
    "   only apply to individual training points.\n",
    "\n",
    "2. **The tree memorizes noise.** Real data always contains some noise (measurement errors,\n",
    "   natural variability). An unrestricted tree will create splits to accommodate this noise,\n",
    "   learning patterns that do not generalize.\n",
    "\n",
    "3. **High model complexity.** A tree with hundreds of leaves is essentially a lookup table.\n",
    "   It has too many parameters relative to the amount of training data.\n",
    "\n",
    "### How depth controls complexity\n",
    "\n",
    "| Depth | Max Leaves | Complexity | Risk |\n",
    "|---|---|---|---|\n",
    "| 1 | 2 | Very low | Underfitting |\n",
    "| 3 | 8 | Low-medium | Usually good |\n",
    "| 5 | 32 | Medium | Often good |\n",
    "| 10 | 1024 | High | Potential overfitting |\n",
    "| None (unlimited) | Up to N | Very high | Almost certain overfitting |\n",
    "\n",
    "The number of possible leaves grows **exponentially** with depth ($2^d$), so even small\n",
    "increases in depth can dramatically increase model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Depth Tuning Experiment\n",
    "\n",
    "Let us systematically train trees at depths 1 through 15 and plot the training and test\n",
    "accuracy curves. We expect to see the classic bias-variance U-curve on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Depth tuning: train trees at depths 1 through 15 ----\n",
    "# We store both train and test accuracy at each depth\n",
    "# so we can later plot the overfitting curve\n",
    "\n",
    "depths = list(range(1, 16))  # Depths from 1 to 15\n",
    "train_accuracies = []        # Will hold training accuracy for each depth\n",
    "test_accuracies = []         # Will hold test accuracy for each depth\n",
    "\n",
    "for d in depths:\n",
    "    # Create a tree constrained to this specific depth\n",
    "    tree_d = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    \n",
    "    # Train on the training set\n",
    "    tree_d.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on both sets\n",
    "    train_acc = accuracy_score(y_train, tree_d.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, tree_d.predict(X_test))\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Display results as a table for easy inspection\n",
    "results_df = pd.DataFrame({\n",
    "    'depth': depths,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'test_accuracy': test_accuracies,\n",
    "    'gap': [tr - te for tr, te in zip(train_accuracies, test_accuracies)]\n",
    "})\n",
    "\n",
    "print(\"Depth Tuning Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find the depth with the best test accuracy\n",
    "best_idx = np.argmax(test_accuracies)\n",
    "best_depth = depths[best_idx]\n",
    "best_test_acc = test_accuracies[best_idx]\n",
    "print(f\"\\nBest test accuracy: {best_test_acc:.4f} at depth = {best_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Plotting the Depth vs Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualization: Train vs Test accuracy as a function of tree depth ----\n",
    "# This plot is one of the most important diagnostics in machine learning.\n",
    "# It shows the bias-variance tradeoff in action.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot training accuracy (should increase monotonically with depth)\n",
    "ax.plot(depths, train_accuracies, 'o-', color='steelblue', linewidth=2,\n",
    "        markersize=7, label='Training Accuracy')\n",
    "\n",
    "# Plot test accuracy (should increase, peak, then potentially decrease)\n",
    "ax.plot(depths, test_accuracies, 's-', color='darkorange', linewidth=2,\n",
    "        markersize=7, label='Test Accuracy')\n",
    "\n",
    "# Mark the optimal depth with a vertical dashed line\n",
    "ax.axvline(x=best_depth, color='green', linestyle='--', alpha=0.7,\n",
    "           label=f'Best Depth = {best_depth}')\n",
    "\n",
    "# Annotate the best test accuracy point\n",
    "ax.annotate(\n",
    "    f'Best: {best_test_acc:.3f}\\n(depth={best_depth})',\n",
    "    xy=(best_depth, best_test_acc),\n",
    "    xytext=(best_depth + 2, best_test_acc - 0.03),\n",
    "    fontsize=11,\n",
    "    arrowprops=dict(arrowstyle='->', color='green'),\n",
    "    color='green',\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "# Add shaded regions to indicate underfitting and overfitting zones\n",
    "ax.axvspan(0.5, 2.5, alpha=0.08, color='blue', label='Underfitting zone')\n",
    "ax.axvspan(8.5, 15.5, alpha=0.08, color='red', label='Overfitting zone')\n",
    "\n",
    "# Labels, title, legend\n",
    "ax.set_xlabel('Tree Depth', fontsize=13)\n",
    "ax.set_ylabel('Accuracy', fontsize=13)\n",
    "ax.set_title('Decision Tree: Training vs Test Accuracy by Depth', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(depths)\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.set_ylim(0.85, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- Training accuracy rises quickly to 1.0 as depth increases (the tree memorizes training data).\")\n",
    "print(\"- Test accuracy peaks at an intermediate depth, then plateaus or drops.\")\n",
    "print(\"- The gap between train and test accuracy widens with depth = classic overfitting signature.\")\n",
    "print(f\"- The optimal depth for this dataset appears to be around {best_depth}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Tree Pruning\n",
    "\n",
    "There are two main approaches to controlling tree complexity:\n",
    "\n",
    "### Pre-Pruning (Early Stopping)\n",
    "Stop growing the tree before it becomes too complex. This is what `max_depth`,\n",
    "`min_samples_split`, and `min_samples_leaf` do.\n",
    "\n",
    "- **Pros**: Simple, fast, easy to tune.\n",
    "- **Cons**: May stop too early and miss useful splits deeper in the tree.\n",
    "\n",
    "### Post-Pruning (Cost-Complexity Pruning)\n",
    "Grow a full tree first, then **remove branches** that do not improve generalization.\n",
    "Scikit-learn implements this via the **`ccp_alpha`** parameter.\n",
    "\n",
    "The idea: for each subtree, compute\n",
    "\n",
    "$$R_{\\alpha}(T) = R(T) + \\alpha \\cdot |T|$$\n",
    "\n",
    "where:\n",
    "- $R(T)$ = total impurity of all leaves (training error)\n",
    "- $|T|$ = number of leaves (complexity)\n",
    "- $\\alpha$ = the complexity parameter (`ccp_alpha`)\n",
    "\n",
    "A higher $\\alpha$ penalizes complexity more heavily, resulting in a smaller tree.\n",
    "When $\\alpha = 0$, we get the full tree. As $\\alpha$ increases, branches are pruned.\n",
    "\n",
    "We can find the optimal $\\alpha$ using **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cost-Complexity Pruning with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cost-Complexity Pruning ----\n",
    "# Step 1: Get the effective alpha values for the full tree\n",
    "# cost_complexity_pruning_path() returns the sequence of alpha values\n",
    "# and corresponding impurities as we prune the tree progressively\n",
    "tree_full = DecisionTreeClassifier(random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "\n",
    "# This gives us the pruning path: a sequence of (alpha, impurity) pairs\n",
    "path = tree_full.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas  # Array of alpha values from 0 (full tree) to max\n",
    "impurities = path.impurities  # Corresponding total leaf impurities\n",
    "\n",
    "print(f\"Number of alpha values to evaluate: {len(ccp_alphas)}\")\n",
    "print(f\"Alpha range: {ccp_alphas.min():.6f} to {ccp_alphas.max():.6f}\")\n",
    "\n",
    "# Step 2: For each alpha, train a tree and evaluate with 5-fold cross-validation\n",
    "# We use cross-validation instead of a single train/test split for more robust estimates\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    tree_cv = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)\n",
    "    # cross_val_score performs 5-fold CV and returns accuracy for each fold\n",
    "    scores = cross_val_score(tree_cv, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores_mean.append(scores.mean())\n",
    "    cv_scores_std.append(scores.std())\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "cv_scores_mean = np.array(cv_scores_mean)\n",
    "cv_scores_std = np.array(cv_scores_std)\n",
    "\n",
    "# Find the best alpha (highest mean CV accuracy)\n",
    "best_alpha_idx = np.argmax(cv_scores_mean)\n",
    "best_alpha = ccp_alphas[best_alpha_idx]\n",
    "print(f\"\\nBest ccp_alpha: {best_alpha:.6f}\")\n",
    "print(f\"Best CV accuracy: {cv_scores_mean[best_alpha_idx]:.4f} (+/- {cv_scores_std[best_alpha_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot CV accuracy vs ccp_alpha ----\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot mean CV accuracy with error band (1 standard deviation)\n",
    "ax.plot(ccp_alphas, cv_scores_mean, 'o-', color='steelblue', markersize=3, label='Mean CV Accuracy')\n",
    "ax.fill_between(ccp_alphas,\n",
    "                cv_scores_mean - cv_scores_std,\n",
    "                cv_scores_mean + cv_scores_std,\n",
    "                alpha=0.2, color='steelblue', label='+/- 1 Std Dev')\n",
    "\n",
    "# Mark the best alpha\n",
    "ax.axvline(x=best_alpha, color='green', linestyle='--', alpha=0.7, label=f'Best alpha={best_alpha:.4f}')\n",
    "\n",
    "ax.set_xlabel('ccp_alpha (Complexity Parameter)', fontsize=12)\n",
    "ax.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Cost-Complexity Pruning: CV Accuracy vs Alpha', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- At alpha=0 (full tree), the tree is complex and may overfit.\")\n",
    "print(\"- As alpha increases, the tree is pruned and CV accuracy first improves, then drops.\")\n",
    "print(f\"- The optimal alpha ({best_alpha:.4f}) balances complexity and performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Optimal Tree\n",
    "\n",
    "We now train the final decision tree using the best depth we found, and also evaluate the\n",
    "pruned tree. We will pick whichever performs better on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train the depth-tuned tree ----\n",
    "tree_depth_tuned = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\n",
    "tree_depth_tuned.fit(X_train, y_train)\n",
    "\n",
    "# ---- Train the pruned tree ----\n",
    "tree_pruned = DecisionTreeClassifier(ccp_alpha=best_alpha, random_state=42)\n",
    "tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "# ---- Evaluate all three trees on the test set ----\n",
    "models = {\n",
    "    'Unrestricted': tree_unrestricted,\n",
    "    f'Depth-Tuned (depth={best_depth})': tree_depth_tuned,\n",
    "    f'Pruned (alpha={best_alpha:.4f})': tree_pruned,\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<35} {'Train Acc':>10} {'Test Acc':>10} {'Depth':>7} {'Leaves':>8}\")\n",
    "print('-' * 72)\n",
    "\n",
    "best_model_name = None\n",
    "best_model_test_acc = 0\n",
    "best_model_obj = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    tr_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    te_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    depth = model.get_depth()\n",
    "    leaves = model.get_n_leaves()\n",
    "    print(f\"{name:<35} {tr_acc:>10.4f} {te_acc:>10.4f} {depth:>7} {leaves:>8}\")\n",
    "    \n",
    "    # Track the best model by test accuracy\n",
    "    if te_acc > best_model_test_acc:\n",
    "        best_model_test_acc = te_acc\n",
    "        best_model_name = name\n",
    "        best_model_obj = model\n",
    "\n",
    "print(f\"\\nBest decision tree model: {best_model_name} (test accuracy: {best_model_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Visualizing Decision Trees\n",
    "\n",
    "One of the biggest advantages of decision trees is **interpretability**. We can literally\n",
    "read the decision rules the model learned.\n",
    "\n",
    "When you visualize a tree, each node shows:\n",
    "\n",
    "| Element | Meaning |\n",
    "|---|---|\n",
    "| **Feature <= threshold** | The splitting condition (samples going left satisfy this) |\n",
    "| **gini** or **entropy** | The impurity of this node |\n",
    "| **samples** | Number of training samples that reached this node |\n",
    "| **value = [a, b]** | Number of samples in each class [malignant, benign] |\n",
    "| **class** | The majority class prediction at this node |\n",
    "\n",
    "**Reading a tree:**\n",
    "- Start at the root (top).\n",
    "- If the condition is True, go **left**. If False, go **right**.\n",
    "- Repeat until you reach a leaf node --- that is the prediction.\n",
    "- The color intensity indicates confidence: darker = more pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visual tree plot ----\n",
    "# We visualize a shallow tree (depth=3) for readability\n",
    "# A full tree would be too wide to display clearly\n",
    "tree_visual = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_visual.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# plot_tree draws the tree structure with all node information\n",
    "# filled=True colors nodes by majority class (helps see the split pattern)\n",
    "# rounded=True makes the boxes look nicer\n",
    "# fontsize controls readability\n",
    "plot_tree(\n",
    "    tree_visual,\n",
    "    feature_names=cancer.feature_names,   # Use real feature names instead of indices\n",
    "    class_names=cancer.target_names,       # Use 'malignant'/'benign' instead of 0/1\n",
    "    filled=True,                           # Color nodes by class\n",
    "    rounded=True,                          # Round box corners\n",
    "    fontsize=9,                            # Font size for node text\n",
    "    ax=ax                                  # Draw on our axes\n",
    ")\n",
    "\n",
    "ax.set_title('Decision Tree Visualization (max_depth=3)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"HOW TO READ THIS TREE:\")\n",
    "print(\"- Start at the root node (top center).\")\n",
    "print(\"- If the condition is TRUE, follow the LEFT branch.\")\n",
    "print(\"- If the condition is FALSE, follow the RIGHT branch.\")\n",
    "print(\"- Orange nodes lean toward malignant; blue nodes lean toward benign.\")\n",
    "print(\"- Darker color = higher purity (more confident prediction).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Text representation of the tree ----\n",
    "# export_text produces a text-based representation that is useful for\n",
    "# documentation, logging, or when you cannot render graphics\n",
    "tree_text = export_text(\n",
    "    tree_visual,\n",
    "    feature_names=list(cancer.feature_names)  # Must be a list, not numpy array\n",
    ")\n",
    "\n",
    "print(\"Text representation of the decision tree (depth=3):\")\n",
    "print(\"=\" * 60)\n",
    "print(tree_text)\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Each indentation level represents one level deeper in the tree.\")\n",
    "print(\"'|---' lines show the path for when the condition is TRUE (left child).\")\n",
    "print(\"'|---' lines after the condition show the path for FALSE (right child).\")\n",
    "print(\"'class:' at the leaf nodes shows the predicted class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Feature Importance in Decision Trees\n",
    "\n",
    "Decision trees provide a natural measure of **feature importance** based on how much each\n",
    "feature contributes to reducing impurity across all splits in the tree.\n",
    "\n",
    "### How it is calculated\n",
    "\n",
    "For each feature, the importance is computed as:\n",
    "\n",
    "$$\\text{Importance}(f) = \\sum_{\\text{nodes splitting on } f} \\frac{n_t}{n} \\cdot \\Delta \\text{impurity}$$\n",
    "\n",
    "where:\n",
    "- $n_t$ = number of samples reaching node $t$\n",
    "- $n$ = total number of training samples\n",
    "- $\\Delta \\text{impurity}$ = reduction in impurity (Gini or entropy) caused by this split\n",
    "\n",
    "The importances are then **normalized to sum to 1.0**.\n",
    "\n",
    "### Limitations to be aware of\n",
    "\n",
    "1. **Bias toward high-cardinality features**: Features with many unique values get more\n",
    "   possible split points, so they may appear more important than they truly are.\n",
    "2. **Correlated features**: If two features are highly correlated, the tree may use one\n",
    "   and completely ignore the other, even though both are informative.\n",
    "3. **Instability**: Small changes in the data can produce very different trees, which\n",
    "   changes the importance rankings. Ensemble methods (Random Forest) are more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Extract and plot feature importances from the best tree ----\n",
    "# We use the best model from our comparison above\n",
    "importances = best_model_obj.feature_importances_  # Array of shape (30,), sums to 1.0\n",
    "\n",
    "# Create a DataFrame for easy sorting and display\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': cancer.feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)  # Sort by importance, highest first\n",
    "\n",
    "# Show only features with non-zero importance\n",
    "# (many features may have importance=0 if the tree never split on them)\n",
    "nonzero = importance_df[importance_df['importance'] > 0]\n",
    "print(f\"Features used by the tree: {len(nonzero)} out of {len(cancer.feature_names)}\")\n",
    "print(f\"Features NOT used: {len(cancer.feature_names) - len(nonzero)}\")\n",
    "print()\n",
    "print(\"Top 10 most important features:\")\n",
    "print(nonzero.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Bar plot of feature importances ----\n",
    "# Show the top 15 features for clarity (plotting all 30 can be cluttered)\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Horizontal bar chart: longest bars at the top\n",
    "bars = ax.barh(\n",
    "    range(len(top_features)),\n",
    "    top_features['importance'].values,\n",
    "    color='steelblue',\n",
    "    edgecolor='white'\n",
    ")\n",
    "\n",
    "# Label each bar with the feature name\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'].values, fontsize=10)\n",
    "ax.invert_yaxis()  # Highest importance at the top\n",
    "\n",
    "# Add value labels on the bars for precise reading\n",
    "for bar_idx, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    if width > 0.01:  # Only label bars that are visible\n",
    "        ax.text(width + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}', va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Feature Importance (Gini reduction)', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Feature Importances ({best_model_name})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(f\"- The most important feature is '{top_features.iloc[0]['feature']}' \"\n",
    "      f\"(importance={top_features.iloc[0]['importance']:.3f}).\")\n",
    "print(\"- 'Worst' features (computed from the largest/most extreme cell nuclei) tend to\")\n",
    "print(\"  dominate because they capture the most extreme differences between malignant and benign.\")\n",
    "print(\"- Many features have zero importance -- the tree found them unnecessary for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Decision Trees vs Logistic Regression\n",
    "\n",
    "Now let us compare the two models we have studied so far.\n",
    "\n",
    "| Aspect | Decision Tree | Logistic Regression |\n",
    "|---|---|---|\n",
    "| **Decision boundary** | Non-linear (axis-aligned rectangles) | Linear (hyperplane) |\n",
    "| **Interpretability** | Very high (readable rules) | Moderate (coefficients + odds ratios) |\n",
    "| **Feature scaling needed?** | No (splits on thresholds) | Yes (gradient-based optimization) |\n",
    "| **Handles non-linear relationships?** | Yes, naturally | No, needs feature engineering |\n",
    "| **Handles feature interactions?** | Yes (nested splits) | No (unless explicitly added) |\n",
    "| **Robustness to outliers** | High (threshold-based splits) | Moderate |\n",
    "| **Stability** | Low (small data changes -> different tree) | High (stable coefficients) |\n",
    "| **Overfitting risk** | High (if unrestricted) | Lower (regularization built in) |\n",
    "| **Probabilistic output** | Crude (leaf proportions) | Smooth (sigmoid function) |\n",
    "| **Training speed** | Fast | Fast |\n",
    "\n",
    "Neither model is universally better. The right choice depends on your data and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train a logistic regression model for head-to-head comparison ----\n",
    "# Logistic regression requires feature scaling because it uses gradient descent\n",
    "# and features are on very different scales (e.g., area ~100-2500 vs smoothness ~0.05-0.16)\n",
    "\n",
    "# Step 1: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data, then transform\n",
    "X_test_scaled = scaler.transform(X_test)          # Only transform test data (no fitting!)\n",
    "\n",
    "# Step 2: Train logistic regression\n",
    "# max_iter=10000 ensures convergence (default 100 may not be enough for 30 features)\n",
    "logreg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Get predictions from both models\n",
    "# Decision tree predictions (using our best tree)\n",
    "tree_preds = best_model_obj.predict(X_test)\n",
    "tree_probs = best_model_obj.predict_proba(X_test)[:, 1]  # Probability of benign class\n",
    "\n",
    "# Logistic regression predictions\n",
    "lr_preds = logreg.predict(X_test_scaled)\n",
    "lr_probs = logreg.predict_proba(X_test_scaled)[:, 1]  # Probability of benign class\n",
    "\n",
    "print(\"Models trained successfully. Ready for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Side-by-side metric comparison ----\n",
    "# We compute the same metrics for both models and display them in a table\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Compute a dictionary of classification metrics.\"\"\"\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "    }\n",
    "\n",
    "# Compute metrics for both models\n",
    "tree_metrics = compute_metrics(y_test, tree_preds, tree_probs)\n",
    "lr_metrics = compute_metrics(y_test, lr_preds, lr_probs)\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': list(tree_metrics.keys()),\n",
    "    f'Decision Tree ({best_model_name})': [f\"{v:.4f}\" for v in tree_metrics.values()],\n",
    "    'Logistic Regression': [f\"{v:.4f}\" for v in lr_metrics.values()],\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HEAD-TO-HEAD COMPARISON: Decision Tree vs Logistic Regression\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Determine winner for each metric\n",
    "for metric in tree_metrics:\n",
    "    tree_val = tree_metrics[metric]\n",
    "    lr_val = lr_metrics[metric]\n",
    "    if abs(tree_val - lr_val) < 0.001:\n",
    "        print(f\"  {metric}: TIE\")\n",
    "    elif tree_val > lr_val:\n",
    "        print(f\"  {metric}: Decision Tree wins (+{tree_val - lr_val:.4f})\")\n",
    "    else:\n",
    "        print(f\"  {metric}: Logistic Regression wins (+{lr_val - tree_val:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Confusion matrices side by side ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, preds, title in zip(\n",
    "    axes,\n",
    "    [tree_preds, lr_preds],\n",
    "    [f'Decision Tree ({best_model_name})', 'Logistic Regression']\n",
    "):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    \n",
    "    # Display as a heatmap using matplotlib\n",
    "    im = ax.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    \n",
    "    # Add text annotations showing the counts in each cell\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            # Choose text color based on background intensity\n",
    "            color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "            ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                    fontsize=20, fontweight='bold', color=color)\n",
    "    \n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "    ax.set_yticklabels(['Malignant', 'Benign'], fontsize=11)\n",
    "    ax.set_xlabel('Predicted', fontsize=12)\n",
    "    ax.set_ylabel('Actual', fontsize=12)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrix Comparison', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"READING THE CONFUSION MATRIX:\")\n",
    "print(\"- Top-left (TN): Correctly identified malignant tumors\")\n",
    "print(\"- Top-right (FP): Malignant tumors incorrectly predicted as benign (DANGEROUS in medicine!)\")\n",
    "print(\"- Bottom-left (FN): Benign tumors incorrectly predicted as malignant\")\n",
    "print(\"- Bottom-right (TP): Correctly identified benign tumors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Strengths and Weaknesses of Decision Trees\n",
    "\n",
    "### Strengths\n",
    "\n",
    "1. **Highly interpretable**: You can read the decision rules. Crucial for regulated industries.\n",
    "2. **No feature scaling required**: Trees split on thresholds, so the absolute scale is irrelevant.\n",
    "3. **Handles non-linear relationships**: Can capture complex patterns that linear models miss.\n",
    "4. **Handles feature interactions**: Nested splits naturally model interactions.\n",
    "5. **Works with both numerical and categorical features** (sklearn requires encoding, but the\n",
    "   algorithm itself handles both).\n",
    "6. **Fast training and prediction**: Even on large datasets.\n",
    "7. **Built-in feature selection**: Only informative features are used for splits.\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "1. **Prone to overfitting**: Without pruning, trees memorize training data.\n",
    "2. **High variance / instability**: Small changes in data can produce a completely different tree.\n",
    "3. **Axis-aligned splits only**: Decision boundaries are always perpendicular to feature axes.\n",
    "   Diagonal boundaries require many splits to approximate.\n",
    "4. **Greedy algorithm**: Each split is locally optimal but may not be globally optimal.\n",
    "5. **Biased toward features with many values**: More split points = more chances to look useful.\n",
    "\n",
    "### Preview: Ensemble Methods (What Comes Next)\n",
    "\n",
    "The weaknesses of individual trees are addressed by **ensemble methods** that combine\n",
    "many trees:\n",
    "\n",
    "| Method | How it works | Fixes |\n",
    "|---|---|---|\n",
    "| **Random Forest** | Trains many trees on random subsets of data and features, averages predictions | Reduces variance, improves stability |\n",
    "| **Gradient Boosting** (XGBoost, LightGBM) | Trains trees sequentially, each correcting the previous one's errors | Reduces bias and variance |\n",
    "| **AdaBoost** | Trains trees on weighted data, upweighting misclassified samples | Reduces bias |\n",
    "\n",
    "These ensemble methods are among the **most powerful** algorithms in machine learning\n",
    "and dominate tabular data competitions (e.g., Kaggle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary of Findings\n",
    "\n",
    "Here is what we learned in this notebook:\n",
    "\n",
    "### Theory\n",
    "- Decision trees learn by **recursive binary splitting**, choosing the (feature, threshold) pair\n",
    "  that maximizes purity at each step.\n",
    "- **Gini impurity** and **entropy** are the two main splitting criteria. They produce similar\n",
    "  results; Gini is marginally faster.\n",
    "- Unrestricted trees **overfit** by memorizing training data. The bias-variance tradeoff tells us\n",
    "  there is an optimal complexity that minimizes test error.\n",
    "- **Pre-pruning** (max_depth, min_samples) and **post-pruning** (ccp_alpha) are the two main\n",
    "  strategies for controlling tree complexity.\n",
    "\n",
    "### Practice\n",
    "- An unrestricted tree achieved **100% training accuracy** but lower test accuracy (overfitting).\n",
    "- Depth tuning revealed a sweet spot, and we visualized the classic train/test accuracy curves.\n",
    "- Cost-complexity pruning with cross-validation provided another way to find the optimal tree.\n",
    "- Feature importance analysis revealed which features the tree relies on most heavily.\n",
    "\n",
    "### Model Comparison\n",
    "- We compared our best decision tree against logistic regression on the same test set.\n",
    "- Both models perform well on this dataset. Logistic regression may have an edge in ROC-AUC\n",
    "  (smoother probability estimates), while the decision tree offers superior interpretability.\n",
    "- The decision tree requires **no feature scaling**, which simplifies preprocessing.\n",
    "\n",
    "### Key Takeaway\n",
    "> A decision tree is a powerful, interpretable model, but it must be carefully constrained\n",
    "> (via depth limits or pruning) to generalize well. Its weaknesses (instability, overfitting)\n",
    "> are addressed by ensemble methods, which we will encounter later in the ML Track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps: Connection to Day 04 (Feature Engineering)\n",
    "\n",
    "In **Day 04**, we will explore **Feature Engineering** --- the art of transforming raw features\n",
    "into more informative representations that help models learn better.\n",
    "\n",
    "Feature engineering is often the single biggest lever for improving model performance.\n",
    "It matters for both decision trees and logistic regression:\n",
    "\n",
    "- **For logistic regression**: Adding polynomial features or interaction terms can help it\n",
    "  capture non-linear relationships (which decision trees handle naturally).\n",
    "- **For decision trees**: Creating ratio features (e.g., `area / perimeter`) can help the tree\n",
    "  find splits that would otherwise require many levels of nesting.\n",
    "\n",
    "We will continue using the Breast Cancer dataset so we can directly compare how feature\n",
    "engineering affects the models we have already built.\n",
    "\n",
    "**See you in Day 04!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
