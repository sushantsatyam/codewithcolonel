{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 03 — Decision Trees + Comparison\n",
        "\n",
        "Decision trees are intuitive and powerful but can overfit if not controlled.\n",
        "\n",
        "We will cover:\n",
        "- How decision trees split data\n",
        "- Gini/entropy intuition\n",
        "- Controlling depth to avoid overfitting\n",
        "- Comparing a decision tree to logistic regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load a real dataset\n",
        "We will use the Breast Cancer Wisconsin dataset from scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Train a decision tree\n",
        "A decision tree splits the data into regions that are as “pure” as possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "train_acc = accuracy_score(y_train, tree.predict(X_train))\n",
        "test_acc = accuracy_score(y_test, tree.predict(X_test))\n",
        "\n",
        "train_acc, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Control overfitting with max_depth\n",
        "Decision trees can memorize the training set. Limiting depth improves generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "depths = list(range(1, 11))\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for d in depths:\n",
        "    model = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    train_scores.append(accuracy_score(y_train, model.predict(X_train)))\n",
        "    test_scores.append(accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "pd.DataFrame({\"depth\": depths, \"train_acc\": train_scores, \"test_acc\": test_scores})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Visualize a small tree\n",
        "Plotting a shallow tree makes the logic interpretable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "small_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "small_tree.fit(X_train, y_train)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plot_tree(small_tree, feature_names=cancer.feature_names, class_names=cancer.target_names, filled=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Compare with logistic regression\n",
        "Logistic regression is a linear model, which can be a strong baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "logreg_acc = accuracy_score(y_test, logreg.predict(X_test))\n",
        "small_tree_acc = accuracy_score(y_test, small_tree.predict(X_test))\n",
        "\n",
        "logreg_acc, small_tree_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) What to do next\n",
        "Next we’ll focus on **feature engineering**, which often yields bigger gains than model changes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}