{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 05 \u2014 Hyperparameter Tuning: Theory + Hands-On Walkthrough\n",
    "\n",
    "Welcome to Day 05 of the ML Track.\n",
    "\n",
    "In Day 02 and Day 03, we trained strong baseline models and learned how to evaluate them.\n",
    "Today, we answer an important question:\n",
    "\n",
    "> **How do we systematically improve a model without manually guessing settings?**\n",
    "\n",
    "The answer is **hyperparameter tuning**.\n",
    "\n",
    "---\n",
    "## Learning goals\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the difference between **model parameters** and **hyperparameters**.\n",
    "2. Use **cross-validation** correctly for model selection.\n",
    "3. Run both **GridSearchCV** and **RandomizedSearchCV**.\n",
    "4. Compare tuned models against a baseline using multiple metrics.\n",
    "5. Interpret search results and choose a practical final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Parameters vs Hyperparameters\n",
    "\n",
    "- **Parameters** are learned from data during training.\n",
    "  - Example (logistic regression): feature weights / coefficients.\n",
    "- **Hyperparameters** are set *before* training.\n",
    "  - Example: regularization strength `C`, solver choice, iteration limit.\n",
    "\n",
    "Hyperparameters control model flexibility, optimization behavior, and generalization.\n",
    "If we choose them poorly, we can underfit, overfit, or train inefficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Why We Need Validation and Cross-Validation\n",
    "\n",
    "If we tune directly on the test set, we leak information and get over-optimistic results.\n",
    "\n",
    "Correct workflow:\n",
    "\n",
    "1. Split data into **train** and **test**.\n",
    "2. Use only the **training split** for tuning.\n",
    "3. Within training data, use **k-fold cross-validation**:\n",
    "   - Train on k-1 folds\n",
    "   - Validate on the remaining fold\n",
    "   - Repeat for all folds and average the score\n",
    "4. Evaluate the selected model **once** on the untouched test set.\n",
    "\n",
    "This keeps the test set as an honest estimate of real-world performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset (Breast Cancer Wisconsin)\n",
    "\n",
    "We keep the same dataset used earlier in the track for apples-to-apples comparison.\n",
    "\n",
    "- **Target classes**:\n",
    "  - `0` = malignant\n",
    "  - `1` = benign\n",
    "- This is a binary classification problem with numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts(normalize=True).rename(\"ratio\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "We reserve 20% for final testing and use `stratify=y` to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline pipeline\n",
    "\n",
    "Why a pipeline?\n",
    "\n",
    "- Prevents data leakage by fitting preprocessing only on training folds.\n",
    "- Keeps steps reusable and clean (`scaler` -> `model`).\n",
    "- Makes tuning easier with namespaced hyperparameters like `model__C`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "y_proba_baseline = baseline_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_baseline),\n",
    "    \"precision\": precision_score(y_test, y_pred_baseline),\n",
    "    \"recall\": recall_score(y_test, y_pred_baseline),\n",
    "    \"f1\": f1_score(y_test, y_pred_baseline),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba_baseline),\n",
    "}\n",
    "\n",
    "pd.Series(baseline_metrics).round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Grid Search vs Randomized Search\n",
    "\n",
    "### Grid Search (`GridSearchCV`)\n",
    "- Exhaustively evaluates every combination in a predefined grid.\n",
    "- Good when search space is small and we want deterministic coverage.\n",
    "- Cost grows quickly as dimensions increase.\n",
    "\n",
    "### Randomized Search (`RandomizedSearchCV`)\n",
    "- Samples a fixed number of combinations (`n_iter`).\n",
    "- Often finds near-optimal settings much faster in larger spaces.\n",
    "- Better compute/performance tradeoff in many real projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cross-validation strategy\n",
    "\n",
    "We use **StratifiedKFold** so each fold keeps class proportions similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search (exhaustive tuning)\n",
    "\n",
    "We tune logistic regression hyperparameters that usually matter most:\n",
    "\n",
    "- `C`: inverse regularization strength\n",
    "- `penalty`: regularization type\n",
    "- `solver`: optimization algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=4000, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    \"model__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=grid_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best GridSearch params:\", grid_search.best_params_)\n",
    "print(\"Best mean CV F1:\", round(grid_search.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect top grid-search configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "columns_to_show = [\n",
    "    \"mean_test_score\",\n",
    "    \"std_test_score\",\n",
    "    \"param_model__C\",\n",
    "    \"param_model__solver\",\n",
    "    \"rank_test_score\",\n",
    "]\n",
    "\n",
    "grid_results[columns_to_show].sort_values(\"rank_test_score\").head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search (faster exploration)\n",
    "\n",
    "Here we sample from a wider `C` range but evaluate only a fixed number of trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=4000, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"model__C\": np.logspace(-4, 3, 200),\n",
    "    \"model__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=random_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=24,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RandomSearch params:\", random_search.best_params_)\n",
    "print(\"Best mean CV F1:\", round(random_search.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate baseline vs tuned models on test data\n",
    "\n",
    "We evaluate:\n",
    "- Baseline model\n",
    "- Best GridSearch model\n",
    "- Best RandomSearch model\n",
    "\n",
    "Using multiple metrics gives a fuller picture than accuracy alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba),\n",
    "    }\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    evaluate_model(\"baseline\", baseline_pipeline, X_test, y_test),\n",
    "    evaluate_model(\"grid_best\", grid_search.best_estimator_, X_test, y_test),\n",
    "    evaluate_model(\"random_best\", random_search.best_estimator_, X_test, y_test),\n",
    "]).set_index(\"model\")\n",
    "\n",
    "comparison.round(4).sort_values(\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: How to choose the final model in practice\n",
    "\n",
    "Do **not** pick based on one metric only.\n",
    "\n",
    "A practical decision framework:\n",
    "\n",
    "1. Prioritize metric aligned with business risk (e.g., recall for missed positives).\n",
    "2. Compare CV score and test score (watch for instability/overfitting).\n",
    "3. Prefer simpler/cheaper models when performance is similar.\n",
    "4. Document why the final model was chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Day 05\n",
    "\n",
    "Today we learned that hyperparameter tuning is a disciplined search process, not trial-and-error guessing.\n",
    "\n",
    "### What we accomplished\n",
    "- Built a robust baseline pipeline.\n",
    "- Tuned logistic regression with both grid and randomized search.\n",
    "- Used stratified 5-fold CV for reliable model selection.\n",
    "- Compared baseline and tuned models on multiple test metrics.\n",
    "\n",
    "### Connection to Day 06\n",
    "In Day 06, we focus on **interpretability**: once a tuned model performs well, we must explain *why* it predicts what it does.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}