{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 02 — Building a Baseline Classification Model\n",
    "\n",
    "## What Is a Baseline Model and Why Every ML Project Needs One\n",
    "\n",
    "A **baseline model** is the simplest reasonable model you build at the very start of a machine learning project. It serves as a **reference point** — a line in the sand that every future model must beat to justify its added complexity.\n",
    "\n",
    "### The \"Beat the Baseline\" Philosophy\n",
    "\n",
    "In professional ML engineering, the workflow looks like this:\n",
    "\n",
    "1. **Understand the problem** — What are we predicting? What data do we have?\n",
    "2. **Build a baseline** — The simplest model that produces reasonable predictions.\n",
    "3. **Iterate and improve** — Try more complex models, feature engineering, hyperparameter tuning.\n",
    "4. **Always compare back to baseline** — If a fancy deep learning model only beats logistic regression by 0.2%, is the added complexity worth it?\n",
    "\n",
    "Without a baseline, you have no way to answer the question: *\"Is my model actually good, or does it just look good?\"*\n",
    "\n",
    "### What We Will Cover in This Notebook\n",
    "\n",
    "- **Theory**: Classification problems, why baselines matter, logistic regression internals, feature scaling, evaluation metrics (accuracy, precision, recall, F1, ROC-AUC), confusion matrices, threshold tuning\n",
    "- **Practice**: Loading a real medical dataset (Breast Cancer Wisconsin), building a logistic regression baseline with scikit-learn Pipelines, evaluating with multiple metrics, plotting ROC and Precision-Recall curves, threshold tuning, and comparing against dummy/random baselines\n",
    "\n",
    "By the end, you will have a fully evaluated baseline model and a deep understanding of *why* each step matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: The Classification Problem\n",
    "\n",
    "Classification is a **supervised learning** task where the goal is to predict a **discrete label** (a category) for each input sample.\n",
    "\n",
    "### Binary vs Multi-Class Classification\n",
    "\n",
    "| Type | # of Classes | Examples |\n",
    "|------|-------------|----------|\n",
    "| **Binary** | 2 | Spam vs Not Spam, Malignant vs Benign, Fraud vs Legitimate |\n",
    "| **Multi-class** | 3+ | Digit recognition (0-9), Sentiment (positive/neutral/negative), Disease type |\n",
    "\n",
    "### Real-World Classification Examples\n",
    "\n",
    "- **Healthcare**: Given tumor measurements, predict whether a tumor is malignant or benign. A false negative (missing cancer) can be fatal, so **recall** is critical.\n",
    "- **Finance**: Given transaction features, predict whether a transaction is fraudulent. Datasets are heavily **imbalanced** (99.9% legitimate), so accuracy alone is meaningless.\n",
    "- **NLP**: Given an email's text, predict whether it is spam. A false positive (marking a real email as spam) annoys users, so **precision** matters.\n",
    "\n",
    "### Decision Boundaries\n",
    "\n",
    "Every classifier learns a **decision boundary** — a line (or surface, in higher dimensions) that separates the classes in feature space. For a simple 2D problem:\n",
    "\n",
    "- **Logistic Regression** learns a straight line (linear boundary)\n",
    "- **Decision Trees** learn axis-aligned rectangular regions\n",
    "- **Neural Networks** learn complex, curved boundaries\n",
    "\n",
    "The key insight: **simpler boundaries generalize better** when you have limited data. This is why logistic regression is an excellent baseline — it is hard to overfit a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Why Baselines Matter\n",
    "\n",
    "There are several levels of baselines, from trivial to useful:\n",
    "\n",
    "### 1. Random Baseline\n",
    "Predict classes completely at random. For a balanced binary problem, this gives ~50% accuracy. **Any real model must beat this.**\n",
    "\n",
    "### 2. Majority Class Baseline (\"Most Frequent\")\n",
    "Always predict the most common class. If 90% of patients are healthy, predicting \"healthy\" for everyone gives 90% accuracy — but catches zero sick patients. **This exposes why accuracy alone is dangerous.**\n",
    "\n",
    "### 3. Simple Model Baseline\n",
    "A straightforward model like logistic regression or a shallow decision tree. This is the **practical baseline** — it uses actual patterns in the data but with minimal complexity.\n",
    "\n",
    "### How Baselines Prevent Wasted Effort\n",
    "\n",
    "Consider this scenario:\n",
    "- You spend 3 weeks building a complex ensemble model with 150 features\n",
    "- It achieves 94% accuracy\n",
    "- You celebrate... until you realize a majority-class prediction gives 93% accuracy\n",
    "- Your complex model is barely better than doing nothing\n",
    "\n",
    "**Baselines give you a reality check.** They tell you:\n",
    "- How much room for improvement exists\n",
    "- Whether your complex model is actually learning useful patterns\n",
    "- What the minimum acceptable performance is\n",
    "\n",
    "> *\"A model is only as good as the baseline it beats.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS — Each library serves a specific purpose in our ML pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# NumPy: The foundation of numerical computing in Python.\n",
    "# We use it for array operations, mathematical functions, and threshold arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Pandas: Data manipulation and analysis library.\n",
    "# Provides DataFrames — tabular data structures that make EDA intuitive.\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib: The foundational plotting library in Python.\n",
    "# We use it for creating ROC curves, PR curves, and threshold plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn: Statistical visualization built on top of matplotlib.\n",
    "# Makes it easy to create informative, attractive confusion matrix heatmaps.\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn.datasets: Provides well-known benchmark datasets.\n",
    "# load_breast_cancer gives us a real-world medical classification dataset.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# sklearn.model_selection: Tools for splitting data and validating models.\n",
    "# train_test_split creates hold-out sets for unbiased evaluation.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn.preprocessing: Data transformation tools.\n",
    "# StandardScaler standardizes features to mean=0, std=1 — critical for logistic regression.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# sklearn.pipeline: Chains preprocessing and model steps together.\n",
    "# Prevents data leakage by ensuring the scaler is fit only on training data.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# sklearn.linear_model: Linear models for classification and regression.\n",
    "# LogisticRegression is our baseline classifier — simple, interpretable, effective.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# sklearn.dummy: \"Dummy\" models that use simple rules (no learning).\n",
    "# DummyClassifier helps us establish the absolute floor for model performance.\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# sklearn.metrics: Functions to evaluate model performance.\n",
    "# We import individual metric functions for fine-grained control.\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,        # Overall correctness: (TP+TN) / total\n",
    "    precision_score,       # Of predicted positives, how many are correct: TP / (TP+FP)\n",
    "    recall_score,          # Of actual positives, how many did we find: TP / (TP+FN)\n",
    "    f1_score,              # Harmonic mean of precision and recall\n",
    "    roc_auc_score,         # Area under the ROC curve\n",
    "    confusion_matrix,      # 2x2 table of TP, TN, FP, FN\n",
    "    classification_report, # Formatted summary of precision, recall, F1 per class\n",
    "    roc_curve,             # Computes FPR and TPR at various thresholds\n",
    "    precision_recall_curve,# Computes precision and recall at various thresholds\n",
    "    average_precision_score# Area under the precision-recall curve\n",
    ")\n",
    "\n",
    "# Configure matplotlib for clean, readable plots\n",
    "plt.rcParams['figure.figsize'] = (8, 5)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 100          # Resolution for notebook display\n",
    "plt.rcParams['font.size'] = 11            # Base font size for readability\n",
    "\n",
    "# Set random seed for reproducibility across the entire notebook.\n",
    "# Anyone running this notebook will get the exact same results.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"All imports successful. Ready to build our baseline model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading the Dataset: Breast Cancer Wisconsin\n",
    "\n",
    "The **Breast Cancer Wisconsin (Diagnostic)** dataset is one of the most widely used datasets in machine learning education. It was created from digitized images of fine needle aspirates (FNA) of breast masses.\n",
    "\n",
    "- **569 samples** (patients)\n",
    "- **30 features** computed from cell nuclei measurements: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension — each with mean, standard error, and \"worst\" (largest) value\n",
    "- **2 target classes**: Malignant (0) and Benign (1)\n",
    "\n",
    "This is a real-world medical classification problem where the stakes are high: missing a malignant tumor (false negative) could cost a life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE BREAST CANCER WISCONSIN DATASET\n",
    "# =============================================================================\n",
    "\n",
    "# load_breast_cancer() returns a Bunch object (similar to a dictionary)\n",
    "# containing the data, target, feature names, target names, and a description.\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "# Convert to a pandas DataFrame for easier exploration and manipulation.\n",
    "# cancer_data.data is a NumPy array of shape (569, 30) — the feature matrix.\n",
    "# cancer_data.feature_names provides human-readable column names.\n",
    "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "\n",
    "# Add the target column to our DataFrame.\n",
    "# 0 = malignant (cancerous), 1 = benign (non-cancerous)\n",
    "df['target'] = cancer_data.target\n",
    "\n",
    "# Create a human-readable label for easier interpretation in outputs.\n",
    "# np.where acts like an if-else: if target==0 -> 'malignant', else -> 'benign'\n",
    "df['diagnosis'] = np.where(df['target'] == 0, 'malignant', 'benign')\n",
    "\n",
    "# Display the first 5 rows to get a feel for the data.\n",
    "# Notice the features are continuous measurements (floats) on very different scales.\n",
    "print(f\"Dataset shape: {df.shape[0]} samples, {df.shape[1] - 2} features\")\n",
    "print(f\"Target classes: {cancer_data.target_names}\")\n",
    "print(f\"\\nFeature names (first 10):\")\n",
    "for i, name in enumerate(cancer_data.feature_names[:10]):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "print(f\"  ... and {len(cancer_data.feature_names) - 10} more\\n\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS — Understanding our data before modeling\n",
    "# =============================================================================\n",
    "\n",
    "# Check the overall shape: how many rows (patients) and columns (features + target)\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of samples (patients): {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 2}\")  # Subtract target and diagnosis columns\n",
    "\n",
    "# Check for missing values — a critical first step in any ML project.\n",
    "# Missing values can break models or silently degrade performance.\n",
    "missing = df.isnull().sum().sum()\n",
    "print(f\"Total missing values: {missing}\")\n",
    "print()  # Blank line for readability\n",
    "\n",
    "# =============================================================================\n",
    "# CLASS DISTRIBUTION — Is the dataset balanced or imbalanced?\n",
    "# This directly affects our choice of evaluation metrics.\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# value_counts() shows how many samples belong to each class\n",
    "class_counts = df['diagnosis'].value_counts()\n",
    "print(class_counts)\n",
    "print()\n",
    "\n",
    "# Calculate the percentage of each class — important for understanding imbalance.\n",
    "# If one class dominates (e.g., 95%), accuracy becomes a misleading metric.\n",
    "class_pct = df['diagnosis'].value_counts(normalize=True) * 100\n",
    "print(\"Class percentages:\")\n",
    "for label, pct in class_pct.items():\n",
    "    print(f\"  {label}: {pct:.1f}%\")\n",
    "\n",
    "# The dataset is moderately imbalanced (~63% benign, ~37% malignant).\n",
    "# This means a majority-class baseline would achieve ~63% accuracy.\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE STATISTICS — Understanding the scale and spread of each feature\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE STATISTICS (first 10 features)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# .describe() gives count, mean, std, min, 25%, 50%, 75%, max for each column.\n",
    "# Notice the wildly different scales: mean radius ~6-28, mean area ~143-2501.\n",
    "# This is why feature scaling is essential for logistic regression.\n",
    "feature_cols = cancer_data.feature_names[:10]  # Show first 10 for readability\n",
    "df[feature_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Train/Test Split\n",
    "\n",
    "### Why Split the Data?\n",
    "\n",
    "The fundamental goal of machine learning is **generalization** — performing well on *new, unseen data*, not just the data we trained on. If we evaluate a model on the same data it was trained on, we get an **overly optimistic** estimate of performance.\n",
    "\n",
    "A **train/test split** partitions the data into:\n",
    "- **Training set**: Used to fit (train) the model. The model sees these examples and learns patterns.\n",
    "- **Test set**: Held back and used *only* for evaluation. The model never sees these during training.\n",
    "\n",
    "### Data Leakage\n",
    "\n",
    "**Data leakage** occurs when information from the test set \"leaks\" into the training process. Common causes:\n",
    "- Fitting a scaler on the full dataset before splitting (the scaler \"knows\" test set statistics)\n",
    "- Using future information as features (e.g., using tomorrow's stock price to predict today's)\n",
    "- Performing feature selection on the full dataset\n",
    "\n",
    "**This is why we use Pipelines** — they ensure the scaler is fit *only* on training data.\n",
    "\n",
    "### Stratification\n",
    "\n",
    "When classes are imbalanced, a random split might put too many of one class in the train set and too few in the test set. **Stratified splitting** ensures both sets have the same class proportions as the original data.\n",
    "\n",
    "### Common Split Ratios\n",
    "\n",
    "| Ratio | Train | Validation | Test | Use Case |\n",
    "|-------|-------|------------|------|----------|\n",
    "| 80/20 | 80% | — | 20% | Simple projects, small datasets |\n",
    "| 60/20/20 | 60% | 20% | 20% | When you need a validation set for tuning |\n",
    "| 70/15/15 | 70% | 15% | 15% | Common in industry |\n",
    "\n",
    "### `random_state` for Reproducibility\n",
    "\n",
    "Setting `random_state=42` (or any fixed integer) ensures the split is identical every time you run the notebook. This is critical for **reproducible research** — anyone running your code gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "# Separate features (X) from the target (y).\n",
    "# X contains the 30 numeric measurements; y contains the binary label (0 or 1).\n",
    "X = df[cancer_data.feature_names]  # Use only the 30 feature columns\n",
    "y = df['target']                    # 0 = malignant, 1 = benign\n",
    "\n",
    "# Perform an 80/20 split with stratification.\n",
    "# stratify=y ensures the class ratio (~37% malignant, ~63% benign) is preserved\n",
    "# in both the training and test sets.\n",
    "# random_state=42 makes the split deterministic and reproducible.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,         # 20% of data reserved for testing\n",
    "    random_state=RANDOM_STATE,  # Reproducibility\n",
    "    stratify=y              # Preserve class proportions in both sets\n",
    ")\n",
    "\n",
    "# Verify the split sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN/TEST SPLIT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "print()\n",
    "\n",
    "# Verify stratification worked correctly.\n",
    "# Both sets should have approximately the same class proportions.\n",
    "print(\"Class distribution verification:\")\n",
    "print(f\"  Overall:  malignant={sum(y==0)/len(y)*100:.1f}%, benign={sum(y==1)/len(y)*100:.1f}%\")\n",
    "print(f\"  Training: malignant={sum(y_train==0)/len(y_train)*100:.1f}%, benign={sum(y_train==1)/len(y_train)*100:.1f}%\")\n",
    "print(f\"  Test:     malignant={sum(y_test==0)/len(y_test)*100:.1f}%, benign={sum(y_test==1)/len(y_test)*100:.1f}%\")\n",
    "print()\n",
    "print(\"Stratification confirmed: class proportions are consistent across splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Logistic Regression\n",
    "\n",
    "Despite its name, **logistic regression** is a **classification** algorithm, not a regression algorithm. It is one of the most important algorithms in machine learning and statistics.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Logistic regression models the **probability** that an input belongs to the positive class:\n",
    "\n",
    "$$P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z = \\mathbf{w}^T \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b$\n",
    "\n",
    "### The Sigmoid Function\n",
    "\n",
    "The function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is called the **sigmoid** (or logistic) function. It maps any real number to the range (0, 1), making the output interpretable as a probability:\n",
    "\n",
    "- When $z \\to +\\infty$: $\\sigma(z) \\to 1$ (confident positive prediction)\n",
    "- When $z = 0$: $\\sigma(z) = 0.5$ (maximum uncertainty)\n",
    "- When $z \\to -\\infty$: $\\sigma(z) \\to 0$ (confident negative prediction)\n",
    "\n",
    "### Log-Odds (Logit)\n",
    "\n",
    "The inverse of the sigmoid is the **logit** function: $z = \\log\\frac{P}{1-P}$. This is the log of the **odds ratio**. Logistic regression is fundamentally a **linear model in log-odds space** — it learns a linear combination of features that predicts the log-odds of the positive class.\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "The default decision rule is: predict class 1 if $P(y=1|\\mathbf{x}) \\geq 0.5$, which corresponds to $z \\geq 0$. This creates a **linear decision boundary** in feature space (a hyperplane).\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Scikit-learn's `LogisticRegression` applies **regularization** by default to prevent overfitting:\n",
    "\n",
    "- **L2 (Ridge)**: Default. Penalizes large weights: $\\text{Cost} + \\lambda \\sum w_i^2$. Shrinks all weights toward zero.\n",
    "- **L1 (Lasso)**: Penalizes with absolute values: $\\text{Cost} + \\lambda \\sum |w_i|$. Can drive some weights to exactly zero (feature selection).\n",
    "- **C parameter**: Inverse of regularization strength. Smaller C = stronger regularization. Default is C=1.0.\n",
    "\n",
    "### Strengths and Weaknesses\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "|-----------|------------|\n",
    "| Fast to train and predict | Assumes linear decision boundary |\n",
    "| Outputs calibrated probabilities | Cannot capture complex non-linear patterns |\n",
    "| Highly interpretable (coefficients = feature importance) | Sensitive to feature scaling |\n",
    "| Works well with small datasets | Sensitive to outliers |\n",
    "| Rarely overfits (with regularization) | Assumes features are roughly independent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Feature Scaling\n",
    "\n",
    "### Why Scaling Matters for Logistic Regression\n",
    "\n",
    "Logistic regression uses **gradient descent** to find optimal weights. When features are on vastly different scales (e.g., `mean radius` ranges 6-28, while `mean area` ranges 143-2501), the loss landscape becomes elongated. This causes:\n",
    "\n",
    "- **Slower convergence**: Gradient descent takes a zigzag path instead of heading straight to the minimum.\n",
    "- **Unfair weight penalization**: Regularization penalizes all weights equally, but a weight for a large-scale feature must be small to produce reasonable outputs. This biases the model.\n",
    "\n",
    "### Types of Scalers\n",
    "\n",
    "| Scaler | Formula | When to Use |\n",
    "|--------|---------|-------------|\n",
    "| **StandardScaler** | $z = \\frac{x - \\mu}{\\sigma}$ | Default choice. Centers to mean=0, std=1. Best for normally distributed features. |\n",
    "| **MinMaxScaler** | $z = \\frac{x - x_{min}}{x_{max} - x_{min}}$ | Scales to [0, 1]. Good when you need bounded values (e.g., neural networks). |\n",
    "| **RobustScaler** | $z = \\frac{x - \\text{median}}{IQR}$ | Uses median and IQR. Robust to outliers — use when your data has extreme values. |\n",
    "\n",
    "### Our Choice: StandardScaler\n",
    "\n",
    "We will use `StandardScaler` because:\n",
    "1. The breast cancer features are approximately normally distributed\n",
    "2. Logistic regression's gradient descent converges fastest with standardized features\n",
    "3. It is the most common default in practice\n",
    "\n",
    "### Preventing Data Leakage with Pipelines\n",
    "\n",
    "We **must** fit the scaler only on training data, then use those same parameters to transform the test data. If we fit on the full dataset, the scaler \"sees\" test data statistics, which constitutes **data leakage**.\n",
    "\n",
    "Scikit-learn's `Pipeline` handles this automatically:\n",
    "- `.fit(X_train)` fits the scaler on training data only\n",
    "- `.transform(X_test)` uses training statistics to transform test data\n",
    "- `.predict(X_test)` chains transform + predict in one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILDING THE BASELINE PIPELINE\n",
    "# =============================================================================\n",
    "# A Pipeline chains preprocessing and modeling steps into a single object.\n",
    "# This is the idiomatic way to build ML models in scikit-learn because:\n",
    "#   1. It prevents data leakage (scaler is fit only on training data)\n",
    "#   2. It simplifies code (one .fit() call does everything)\n",
    "#   3. It makes deployment easier (one object to serialize/save)\n",
    "\n",
    "baseline_model = Pipeline([\n",
    "    # Step 1: StandardScaler\n",
    "    # Transforms each feature to have mean=0 and standard deviation=1.\n",
    "    # This ensures all 30 features contribute equally to the model,\n",
    "    # regardless of their original scale.\n",
    "    ('scaler', StandardScaler()),\n",
    "    \n",
    "    # Step 2: LogisticRegression\n",
    "    # Our baseline classifier. Key parameters:\n",
    "    #   - C=1.0 (default): moderate regularization strength\n",
    "    #   - penalty='l2' (default): Ridge regularization to prevent overfitting\n",
    "    #   - solver='lbfgs' (default): efficient optimizer for small-medium datasets\n",
    "    #   - max_iter=10000: increase from default 100 to ensure convergence\n",
    "    #     with 30 features (more features = more iterations needed)\n",
    "    #   - random_state: reproducibility of the optimization\n",
    "    ('logreg', LogisticRegression(\n",
    "        max_iter=10000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Display the pipeline structure.\n",
    "# Scikit-learn provides a nice representation of the pipeline steps.\n",
    "print(\"Pipeline structure:\")\n",
    "print(baseline_model)\n",
    "print()\n",
    "print(\"Step 1: StandardScaler (mean=0, std=1 normalization)\")\n",
    "print(\"Step 2: LogisticRegression (L2 regularized, C=1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING THE MODEL\n",
    "# =============================================================================\n",
    "# When we call .fit(), the following happens internally:\n",
    "#\n",
    "# 1. StandardScaler.fit_transform(X_train):\n",
    "#    - Computes the mean and std of each of the 30 features using ONLY X_train\n",
    "#    - Transforms X_train: each value becomes (value - mean) / std\n",
    "#    - The scaler stores the means and stds for later use on test data\n",
    "#\n",
    "# 2. LogisticRegression.fit(X_train_scaled, y_train):\n",
    "#    - Initializes 30 weights (one per feature) and 1 bias term\n",
    "#    - Uses L-BFGS optimizer to minimize the log-loss (cross-entropy) function\n",
    "#    - Iteratively adjusts weights to maximize the likelihood of correct predictions\n",
    "#    - Applies L2 regularization to prevent any single weight from becoming too large\n",
    "#    - Stops when convergence criteria are met or max_iter is reached\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# After training, we can inspect what the model learned.\n",
    "# Access the logistic regression step from the pipeline.\n",
    "logreg = baseline_model.named_steps['logreg']\n",
    "\n",
    "# The model learned 30 coefficients (weights) — one for each feature.\n",
    "# Positive weight = feature increases probability of benign (class 1).\n",
    "# Negative weight = feature increases probability of malignant (class 0).\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Number of coefficients: {logreg.coef_.shape[1]}\")\n",
    "print(f\"Number of iterations to converge: {logreg.n_iter_[0]}\")\n",
    "print()\n",
    "\n",
    "# Show the top 5 most influential features (by absolute coefficient value).\n",
    "# Larger absolute values indicate features the model relies on most heavily.\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': cancer_data.feature_names,\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})\n",
    "coef_df['abs_coefficient'] = coef_df['coefficient'].abs()\n",
    "coef_df = coef_df.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 most influential features:\")\n",
    "for i, row in coef_df.head(10).iterrows():\n",
    "    direction = \"-> benign\" if row['coefficient'] > 0 else \"-> malignant\"\n",
    "    print(f\"  {row['feature']:30s} coef={row['coefficient']:+.4f}  {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Classification Metrics Deep Dive\n",
    "\n",
    "A single number cannot capture all aspects of a classifier's performance. We need **multiple metrics**, each highlighting a different aspect.\n",
    "\n",
    "### The Confusion Matrix Foundation\n",
    "\n",
    "All classification metrics are derived from four fundamental counts:\n",
    "\n",
    "| | **Predicted Positive** | **Predicted Negative** |\n",
    "|---|---|---|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "### Metric Formulas\n",
    "\n",
    "| Metric | Formula | Intuition | Prioritize When... |\n",
    "|--------|---------|-----------|--------------------|\n",
    "| **Accuracy** | $\\frac{TP + TN}{TP + TN + FP + FN}$ | Overall correctness | Classes are balanced |\n",
    "| **Precision** | $\\frac{TP}{TP + FP}$ | Of those we predicted positive, how many are correct? | False positives are costly (spam detection) |\n",
    "| **Recall (Sensitivity)** | $\\frac{TP}{TP + FN}$ | Of all actual positives, how many did we find? | False negatives are costly (cancer detection) |\n",
    "| **Specificity** | $\\frac{TN}{TN + FP}$ | Of all actual negatives, how many did we correctly identify? | Complement to recall for the negative class |\n",
    "| **F1 Score** | $2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Balanced combination of precision and recall | You need a single metric balancing both |\n",
    "\n",
    "### Why F1 Uses the Harmonic Mean (Not Arithmetic Mean)\n",
    "\n",
    "The **harmonic mean** penalizes extreme imbalances. Consider:\n",
    "- Precision = 1.0, Recall = 0.0\n",
    "- Arithmetic mean = 0.5 (looks decent!)\n",
    "- Harmonic mean (F1) = 0.0 (correctly shows the model is useless)\n",
    "\n",
    "The harmonic mean ensures that **both** precision and recall must be high for F1 to be high.\n",
    "\n",
    "### When Accuracy Is Misleading\n",
    "\n",
    "Imagine a fraud detection dataset with 99.9% legitimate transactions. A model that always predicts \"legitimate\" achieves 99.9% accuracy but catches **zero fraud**. Accuracy is misleading whenever classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: The Confusion Matrix\n",
    "\n",
    "The confusion matrix is a **visual summary** of a classifier's predictions, organized as a 2x2 table (for binary classification).\n",
    "\n",
    "### Visual Layout\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Neg       Pos\n",
    "Actual  Neg  [  TN    |   FP  ]\n",
    "        Pos  [  FN    |   TP  ]\n",
    "```\n",
    "\n",
    "### Interpreting Each Quadrant\n",
    "\n",
    "- **True Negatives (TN)**: Correctly identified negatives. \"We said it's benign, and it IS benign.\"\n",
    "- **False Positives (FP)** — **Type I Error**: Incorrectly flagged as positive. \"We said it's malignant, but it's actually benign.\" Consequence: unnecessary biopsies, patient anxiety.\n",
    "- **False Negatives (FN)** — **Type II Error**: Missed positives. \"We said it's benign, but it's actually malignant.\" Consequence: **missed cancer** — potentially fatal.\n",
    "- **True Positives (TP)**: Correctly identified positives. \"We said it's malignant, and it IS malignant.\"\n",
    "\n",
    "### Type I vs Type II Errors in Context\n",
    "\n",
    "| Error Type | Formal Name | In Our Context | Severity |\n",
    "|-----------|-------------|----------------|----------|\n",
    "| Type I (FP) | False Alarm | Benign tumor flagged as malignant | Moderate — unnecessary follow-up |\n",
    "| Type II (FN) | Missed Detection | Malignant tumor missed as benign | **Critical — missed cancer** |\n",
    "\n",
    "In medical diagnosis, **Type II errors are usually far more dangerous than Type I errors.** We would rather have some false alarms than miss a single cancer case. This is why recall (sensitivity) is often the most important metric in healthcare applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATING THE BASELINE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Generate predictions on the test set.\n",
    "# .predict() internally: scales X_test using training statistics, then classifies.\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# .predict_proba() returns probability estimates for each class.\n",
    "# Shape: (n_samples, 2) — column 0 is P(malignant), column 1 is P(benign).\n",
    "# We take column 1 (probability of benign/positive class) for ROC and PR curves.\n",
    "y_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE ALL METRICS\n",
    "# =============================================================================\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)    # Precision for the positive class (benign=1)\n",
    "rec = recall_score(y_test, y_pred)         # Recall for the positive class\n",
    "f1 = f1_score(y_test, y_pred)              # Harmonic mean of precision and recall\n",
    "auc = roc_auc_score(y_test, y_proba)       # AUC uses probabilities, not hard predictions\n",
    "\n",
    "# Print all metrics in a clean, organized format.\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {acc:.4f}  (Overall correctness)\")\n",
    "print(f\"Precision: {prec:.4f}  (Of predicted benign, how many are correct?)\")\n",
    "print(f\"Recall:    {rec:.4f}  (Of actual benign cases, how many did we find?)\")\n",
    "print(f\"F1 Score:  {f1:.4f}  (Harmonic mean of precision and recall)\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}  (Probability ranking quality)\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "# Compute the confusion matrix: a 2x2 array of [TN, FP; FN, TP]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()  # Unpack into individual values\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True Negatives  (correctly identified malignant): {tn}\")\n",
    "print(f\"False Positives (benign misclassified as malignant): {fp}\")\n",
    "print(f\"False Negatives (malignant misclassified as benign): {fn}\")\n",
    "print(f\"True Positives  (correctly identified benign): {tp}\")\n",
    "print()\n",
    "\n",
    "# Visualize the confusion matrix as a heatmap for intuitive interpretation.\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,         # Show numbers in each cell\n",
    "    fmt='d',            # Integer format (not scientific notation)\n",
    "    cmap='Blues',        # Blue color scheme\n",
    "    xticklabels=['Malignant (0)', 'Benign (1)'],\n",
    "    yticklabels=['Malignant (0)', 'Benign (1)'],\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('Confusion Matrix — Logistic Regression Baseline')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FULL CLASSIFICATION REPORT\n",
    "# =============================================================================\n",
    "# classification_report provides precision, recall, F1 for EACH class,\n",
    "# plus macro/weighted averages.\n",
    "print(\"=\" * 60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=['malignant (0)', 'benign (1)']\n",
    "))\n",
    "\n",
    "# Interpretation:\n",
    "# - High recall for benign means we correctly identify most benign tumors.\n",
    "# - Check recall for malignant — missing malignant cases is the most dangerous error.\n",
    "# - Compare precision across classes to understand where the model makes mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: ROC Curve & AUC\n",
    "\n",
    "### What the ROC Curve Plots\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve plots:\n",
    "- **X-axis**: False Positive Rate (FPR) = FP / (FP + TN) — the fraction of negatives incorrectly classified as positive\n",
    "- **Y-axis**: True Positive Rate (TPR) = TP / (TP + FN) — same as recall/sensitivity\n",
    "\n",
    "Each point on the curve corresponds to a **different probability threshold**. By sweeping the threshold from 1.0 (predict nothing as positive) to 0.0 (predict everything as positive), we trace out the entire curve.\n",
    "\n",
    "### AUC: Area Under the ROC Curve\n",
    "\n",
    "The **AUC** summarizes the ROC curve in a single number:\n",
    "\n",
    "| AUC Value | Interpretation |\n",
    "|-----------|---------------|\n",
    "| 1.0 | Perfect classifier — separates all positives from negatives |\n",
    "| 0.9-1.0 | Excellent |\n",
    "| 0.8-0.9 | Good |\n",
    "| 0.7-0.8 | Fair |\n",
    "| 0.5 | Random guessing — no discrimination ability |\n",
    "| < 0.5 | Worse than random (model has learned inverted patterns) |\n",
    "\n",
    "**Intuition**: AUC = the probability that the model ranks a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "### When AUC Is Useful vs Misleading\n",
    "\n",
    "- **Useful**: When you care about the model's ability to *rank* examples (e.g., which patients to screen first)\n",
    "- **Potentially misleading**: With highly imbalanced data, because FPR can be low even with many false positives (since TN is huge). In such cases, the Precision-Recall curve is often more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROC CURVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Compute ROC curve data points.\n",
    "# roc_curve returns three arrays:\n",
    "#   fpr: False Positive Rate at each threshold\n",
    "#   tpr: True Positive Rate (Recall) at each threshold\n",
    "#   thresholds: The probability thresholds used\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Create the ROC plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the ROC curve itself\n",
    "ax.plot(\n",
    "    fpr, tpr,\n",
    "    color='darkorange',\n",
    "    linewidth=2,\n",
    "    label=f'Logistic Regression (AUC = {auc:.4f})'\n",
    ")\n",
    "\n",
    "# Plot the diagonal \"random chance\" line.\n",
    "# A model with no discrimination power follows this line (AUC = 0.5).\n",
    "# Everything above this line indicates the model has learned something useful.\n",
    "ax.plot(\n",
    "    [0, 1], [0, 1],\n",
    "    color='navy',\n",
    "    linewidth=1.5,\n",
    "    linestyle='--',\n",
    "    label='Random Chance (AUC = 0.5)'\n",
    ")\n",
    "\n",
    "# Shade the area under the ROC curve to visually represent AUC.\n",
    "ax.fill_between(fpr, tpr, alpha=0.1, color='darkorange')\n",
    "\n",
    "# Label axes and add title\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity / Recall)', fontsize=12)\n",
    "ax.set_title('ROC Curve — Logistic Regression Baseline', fontsize=14)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.grid(True, alpha=0.3)  # Light grid for readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(f\"ROC-AUC = {auc:.4f}\")\n",
    "print(\"Interpretation: The model has a {:.1f}% chance of ranking a randomly\".format(auc * 100))\n",
    "print(\"chosen benign sample higher than a randomly chosen malignant sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Precision-Recall Curve\n",
    "\n",
    "### When to Use PR Curve vs ROC Curve\n",
    "\n",
    "The **Precision-Recall (PR) curve** plots precision (y-axis) against recall (x-axis) at various thresholds. It is especially useful when:\n",
    "\n",
    "1. **Classes are imbalanced**: In imbalanced datasets, ROC curves can be overly optimistic because FPR stays low even with many false positives (since TN is very large). The PR curve directly shows the tradeoff between precision and recall without being inflated by true negatives.\n",
    "\n",
    "2. **You care about the positive class**: ROC gives equal weight to both classes, while PR focuses specifically on how well the model identifies the positive class.\n",
    "\n",
    "### Average Precision (AP)\n",
    "\n",
    "**Average Precision** summarizes the PR curve as the weighted mean of precisions at each threshold, with the increase in recall as the weight:\n",
    "\n",
    "$$AP = \\sum_n (R_n - R_{n-1}) \\cdot P_n$$\n",
    "\n",
    "AP ranges from 0 to 1. A perfect classifier has AP = 1.0. The baseline for a random classifier is the proportion of positives in the dataset.\n",
    "\n",
    "### PR Curve vs ROC Curve — Summary\n",
    "\n",
    "| Aspect | ROC Curve | PR Curve |\n",
    "|--------|-----------|----------|\n",
    "| Best for | Balanced datasets | Imbalanced datasets |\n",
    "| X-axis | FPR | Recall |\n",
    "| Y-axis | TPR (Recall) | Precision |\n",
    "| Random baseline | Diagonal line (AUC=0.5) | Horizontal line at prevalence |\n",
    "| Sensitive to class imbalance | Less sensitive | More sensitive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRECISION-RECALL CURVE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Compute precision-recall curve data points.\n",
    "# precision_recall_curve returns:\n",
    "#   precisions: Precision values at each threshold\n",
    "#   recalls: Recall values at each threshold\n",
    "#   pr_thresholds: The probability thresholds used\n",
    "precisions, recalls, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Average Precision: single-number summary of the PR curve.\n",
    "# Analogous to AUC for the ROC curve.\n",
    "ap = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Create the PR curve plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "ax.plot(\n",
    "    recalls, precisions,\n",
    "    color='green',\n",
    "    linewidth=2,\n",
    "    label=f'Logistic Regression (AP = {ap:.4f})'\n",
    ")\n",
    "\n",
    "# Plot the baseline: a horizontal line at the prevalence of the positive class.\n",
    "# For a random classifier, precision = proportion of positives at all recall levels.\n",
    "prevalence = y_test.sum() / len(y_test)\n",
    "ax.axhline(\n",
    "    y=prevalence,\n",
    "    color='navy',\n",
    "    linewidth=1.5,\n",
    "    linestyle='--',\n",
    "    label=f'Random Baseline (prevalence = {prevalence:.3f})'\n",
    ")\n",
    "\n",
    "# Shade the area under the curve\n",
    "ax.fill_between(recalls, precisions, alpha=0.1, color='green')\n",
    "\n",
    "# Label axes and add title\n",
    "ax.set_xlabel('Recall (Sensitivity)', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve — Logistic Regression Baseline', fontsize=14)\n",
    "ax.legend(loc='lower left', fontsize=11)\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (AP) = {ap:.4f}\")\n",
    "print(f\"Random baseline precision = {prevalence:.4f} (proportion of positive class)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Threshold Tuning\n",
    "\n",
    "### The Default Threshold\n",
    "\n",
    "Logistic regression outputs a probability $P(y=1|\\mathbf{x})$ for each sample. By default, scikit-learn uses a threshold of **0.5**:\n",
    "- If $P \\geq 0.5$: predict class 1 (benign)\n",
    "- If $P < 0.5$: predict class 0 (malignant)\n",
    "\n",
    "But 0.5 is **not always the best threshold**. The optimal threshold depends on the **business context**.\n",
    "\n",
    "### Business-Driven Threshold Selection\n",
    "\n",
    "| Scenario | Adjust Threshold | Effect |\n",
    "|----------|-----------------|--------|\n",
    "| Cancer screening (minimize missed cancers) | **Lower** the threshold (e.g., 0.3) | More samples flagged as malignant; higher recall, lower precision |\n",
    "| Spam filtering (minimize blocking real emails) | **Raise** the threshold (e.g., 0.7) | Fewer samples flagged as spam; higher precision, lower recall |\n",
    "\n",
    "### The Precision-Recall Tradeoff\n",
    "\n",
    "There is a fundamental tradeoff:\n",
    "- **Lowering the threshold**: Catches more true positives (recall goes up) but also catches more false positives (precision goes down)\n",
    "- **Raising the threshold**: Reduces false positives (precision goes up) but misses more true positives (recall goes down)\n",
    "\n",
    "The \"optimal\" threshold is the one that achieves the best balance **for your specific use case**. There is no universal best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THRESHOLD TUNING — Finding the optimal decision threshold\n",
    "# =============================================================================\n",
    "\n",
    "# We will evaluate many thresholds to see how precision, recall, and F1 change.\n",
    "# np.arange creates a range of thresholds from 0.1 to 0.9 in steps of 0.05.\n",
    "thresholds_to_test = np.arange(0.10, 0.95, 0.05)\n",
    "\n",
    "# Store results for each threshold so we can plot and compare.\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    # Apply the custom threshold to probability predictions.\n",
    "    # Instead of the default 0.5, we classify as positive (benign=1)\n",
    "    # only if the predicted probability exceeds our custom threshold.\n",
    "    y_pred_custom = (y_proba >= thresh).astype(int)\n",
    "    \n",
    "    # Compute metrics at this threshold.\n",
    "    # zero_division=0 handles the edge case where precision is undefined (no positive predictions).\n",
    "    p = precision_score(y_test, y_pred_custom, zero_division=0)\n",
    "    r = recall_score(y_test, y_pred_custom, zero_division=0)\n",
    "    f = f1_score(y_test, y_pred_custom, zero_division=0)\n",
    "    a = accuracy_score(y_test, y_pred_custom)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': thresh,\n",
    "        'precision': p,\n",
    "        'recall': r,\n",
    "        'f1': f,\n",
    "        'accuracy': a\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy analysis and display\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find the threshold that maximizes F1 score.\n",
    "# F1 is a good default choice because it balances precision and recall.\n",
    "best_idx = thresh_df['f1'].idxmax()\n",
    "best_threshold = thresh_df.loc[best_idx, 'threshold']\n",
    "best_f1 = thresh_df.loc[best_idx, 'f1']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THRESHOLD TUNING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(thresh_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(f\"\\nOptimal threshold (max F1): {best_threshold:.2f} with F1 = {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE PRECISION / RECALL / F1 vs THRESHOLD\n",
    "# =============================================================================\n",
    "# This plot makes the precision-recall tradeoff visually obvious.\n",
    "# You can clearly see where the curves cross and where F1 peaks.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each metric as a function of the threshold\n",
    "ax.plot(thresh_df['threshold'], thresh_df['precision'],\n",
    "        'b-o', label='Precision', linewidth=2, markersize=4)\n",
    "ax.plot(thresh_df['threshold'], thresh_df['recall'],\n",
    "        'r-s', label='Recall', linewidth=2, markersize=4)\n",
    "ax.plot(thresh_df['threshold'], thresh_df['f1'],\n",
    "        'g-^', label='F1 Score', linewidth=2, markersize=4)\n",
    "\n",
    "# Mark the optimal threshold (max F1) with a vertical line.\n",
    "# This is the threshold that best balances precision and recall.\n",
    "ax.axvline(x=best_threshold, color='gray', linestyle='--', linewidth=1.5,\n",
    "           label=f'Optimal Threshold = {best_threshold:.2f}')\n",
    "\n",
    "# Mark the default threshold at 0.5 for comparison\n",
    "ax.axvline(x=0.5, color='orange', linestyle=':', linewidth=1.5,\n",
    "           label='Default Threshold = 0.50')\n",
    "\n",
    "ax.set_xlabel('Decision Threshold', fontsize=12)\n",
    "ax.set_ylabel('Metric Value', fontsize=12)\n",
    "ax.set_title('Precision, Recall, and F1 vs Decision Threshold', fontsize=14)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.set_xlim([0.1, 0.9])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation guidance\n",
    "print(\"How to read this plot:\")\n",
    "print(\"- As threshold increases: precision goes UP (fewer false positives)\")\n",
    "print(\"- As threshold increases: recall goes DOWN (more false negatives)\")\n",
    "print(\"- F1 peaks where precision and recall are best balanced\")\n",
    "print(f\"- The optimal F1 threshold ({best_threshold:.2f}) may differ from the default (0.50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Theory: Dummy/Random Baselines\n",
    "\n",
    "Before celebrating our logistic regression results, we should ask: **\"How much better is our model compared to doing nothing intelligent at all?\"**\n",
    "\n",
    "Scikit-learn provides `DummyClassifier` — a \"model\" that ignores the features entirely and uses simple rules to make predictions.\n",
    "\n",
    "### DummyClassifier Strategies\n",
    "\n",
    "| Strategy | Behavior | Expected Accuracy |\n",
    "|----------|----------|-------------------|\n",
    "| `most_frequent` | Always predicts the majority class | = majority class proportion |\n",
    "| `stratified` | Predicts classes randomly, maintaining training class proportions | ~ overall accuracy by chance |\n",
    "| `uniform` | Predicts each class with equal probability (50/50 for binary) | ~ 50% for binary |\n",
    "\n",
    "### Why Compare Against Dummy Classifiers?\n",
    "\n",
    "1. **Sanity check**: If your model doesn't beat `most_frequent`, it hasn't learned anything useful.\n",
    "2. **Quantify improvement**: Instead of saying \"my model is 95% accurate,\" you can say \"my model is 32 percentage points better than the naive baseline.\"\n",
    "3. **Expose imbalanced data issues**: A `most_frequent` baseline on imbalanced data can have surprisingly high accuracy, revealing that accuracy alone is insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARING AGAINST RANDOM / DUMMY BASELINES\n",
    "# =============================================================================\n",
    "# We will train three dummy classifiers and compare them to our logistic regression.\n",
    "# This gives us a complete picture of how much value our model actually adds.\n",
    "\n",
    "# Strategy 1: \"most_frequent\" — always predicts the majority class (benign)\n",
    "# This is the simplest possible \"model.\" If we can't beat this, we have a problem.\n",
    "dummy_most_frequent = DummyClassifier(\n",
    "    strategy='most_frequent',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "dummy_most_frequent.fit(X_train, y_train)\n",
    "y_pred_mf = dummy_most_frequent.predict(X_test)\n",
    "\n",
    "# Strategy 2: \"stratified\" — randomly predicts classes based on training distribution\n",
    "# Simulates an \"informed random guesser\" who knows the class proportions.\n",
    "dummy_stratified = DummyClassifier(\n",
    "    strategy='stratified',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "dummy_stratified.fit(X_train, y_train)\n",
    "y_pred_strat = dummy_stratified.predict(X_test)\n",
    "\n",
    "# Strategy 3: \"uniform\" — predicts each class with equal probability (coin flip)\n",
    "# The most \"ignorant\" baseline — doesn't even know the class distribution.\n",
    "dummy_uniform = DummyClassifier(\n",
    "    strategy='uniform',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "dummy_uniform.fit(X_train, y_train)\n",
    "y_pred_unif = dummy_uniform.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPUTE METRICS FOR ALL MODELS\n",
    "# =============================================================================\n",
    "# Build a comparison table with all models side by side.\n",
    "# This is the clearest way to demonstrate the value of our baseline model.\n",
    "\n",
    "def compute_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Compute all classification metrics for a given set of predictions.\n",
    "    Returns a dictionary suitable for building a comparison DataFrame.\"\"\"\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# Compute metrics for each model\n",
    "comparison = pd.DataFrame([\n",
    "    compute_metrics(y_test, y_pred, 'Logistic Regression (Baseline)'),\n",
    "    compute_metrics(y_test, y_pred_mf, 'Dummy: Most Frequent'),\n",
    "    compute_metrics(y_test, y_pred_strat, 'Dummy: Stratified'),\n",
    "    compute_metrics(y_test, y_pred_unif, 'Dummy: Uniform'),\n",
    "])\n",
    "\n",
    "# Display the comparison table\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON — Logistic Regression vs Dummy Baselines\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print()\n",
    "\n",
    "# Calculate the improvement over the best dummy baseline\n",
    "best_dummy_acc = max(\n",
    "    accuracy_score(y_test, y_pred_mf),\n",
    "    accuracy_score(y_test, y_pred_strat),\n",
    "    accuracy_score(y_test, y_pred_unif)\n",
    ")\n",
    "improvement = acc - best_dummy_acc\n",
    "print(f\"Logistic Regression accuracy: {acc:.4f}\")\n",
    "print(f\"Best dummy accuracy:          {best_dummy_acc:.4f}\")\n",
    "print(f\"Improvement over dummy:       +{improvement:.4f} ({improvement*100:.1f} percentage points)\")\n",
    "print()\n",
    "print(\"Our logistic regression baseline significantly outperforms all dummy classifiers,\")\n",
    "print(\"confirming that the model has learned meaningful patterns from the features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUAL COMPARISON — Bar chart of all models\n",
    "# =============================================================================\n",
    "# A visual comparison makes it instantly clear how much better our model is.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data for grouped bar chart.\n",
    "# We compare all four metrics across all four models.\n",
    "models = comparison['Model']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "x = np.arange(len(models))  # x positions for each model\n",
    "width = 0.2                  # Width of each bar\n",
    "\n",
    "# Plot bars for each metric, offset by width for grouping\n",
    "for i, metric in enumerate(metrics):\n",
    "    bars = ax.bar(x + i * width, comparison[metric], width, label=metric)\n",
    "\n",
    "# Customize the plot for readability\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Comparison: Logistic Regression vs Dummy Baselines', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)  # Center tick labels under grouped bars\n",
    "ax.set_xticklabels(models, rotation=15, ha='right', fontsize=9)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')  # Only horizontal gridlines\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The chart clearly shows the logistic regression baseline outperforms\")\n",
    "print(\"all dummy classifiers across every metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "Here is a consolidated view of everything we built and measured in this notebook:\n",
    "\n",
    "### Dataset\n",
    "- **Breast Cancer Wisconsin (Diagnostic)**: 569 samples, 30 features, 2 classes\n",
    "- **Class distribution**: ~63% benign, ~37% malignant (moderately imbalanced)\n",
    "- **Split**: 80% training (455 samples), 20% test (114 samples), stratified\n",
    "\n",
    "### Models Built\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| **Logistic Regression** | StandardScaler + L2-regularized LogisticRegression in a Pipeline |\n",
    "| **Dummy: Most Frequent** | Always predicts benign (majority class) |\n",
    "| **Dummy: Stratified** | Random predictions maintaining class proportions |\n",
    "| **Dummy: Uniform** | Coin-flip random predictions |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **The logistic regression baseline is strong**: It significantly outperforms all dummy classifiers, confirming it has learned real patterns.\n",
    "2. **Multiple metrics matter**: Accuracy alone does not tell the full story, especially with imbalanced classes.\n",
    "3. **Threshold tuning can improve performance**: The default 0.5 threshold is not always optimal. Adjusting it based on the business problem (e.g., minimizing missed cancers) can yield better real-world outcomes.\n",
    "4. **Pipelines prevent data leakage**: By bundling the scaler and model together, we ensure test data is never used during training.\n",
    "5. **Dummy baselines set the floor**: Any model we build in future notebooks must beat the logistic regression baseline to justify its complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Connection to Day 03\n",
    "\n",
    "We now have a **well-evaluated baseline** that future models must beat. In **Day 03**, we will explore **Decision Trees**, which offer:\n",
    "\n",
    "- **Non-linear decision boundaries**: Unlike logistic regression's straight-line boundary, decision trees can capture complex, non-linear relationships between features.\n",
    "- **Built-in feature selection**: Decision trees automatically determine which features are most informative by choosing the best splits.\n",
    "- **No feature scaling required**: Decision trees split on individual feature values, so they are invariant to the scale of features.\n",
    "- **High interpretability**: You can visualize the entire tree and trace exactly *why* a particular prediction was made.\n",
    "\n",
    "However, decision trees also introduce new challenges:\n",
    "- **Overfitting**: A deep tree memorizes the training data. We will learn about pruning and depth limits.\n",
    "- **Instability**: Small changes in data can produce very different trees. This motivates ensemble methods (Random Forests, covered later).\n",
    "\n",
    "### The ML Workflow So Far\n",
    "\n",
    "```\n",
    "Day 01: Data Preparation & EDA\n",
    "Day 02: Baseline Model (Logistic Regression)    <-- You are here\n",
    "Day 03: Decision Trees (non-linear models)\n",
    "Day 04: Ensemble Methods (Random Forest, Boosting)\n",
    "  ...\n",
    "```\n",
    "\n",
    "Every model we build from here on will be compared against today's logistic regression baseline. If a more complex model does not meaningfully improve on these results, the simpler model wins — that is the **beat the baseline** philosophy in action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}