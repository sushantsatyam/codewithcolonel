{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 02 — Baseline model (Logistic Regression)\n",
        "\n",
        "In this notebook we build a **baseline classifier**. A baseline gives you a reference point so you can tell if future models are actually better.\n",
        "\n",
        "We will cover:\n",
        "- What a baseline model is\n",
        "- Train/test splitting\n",
        "- Logistic regression for binary classification\n",
        "- Core evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
        "- Simple threshold tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Create a small classification dataset\n",
        "We’ll reuse a tiny, interpretable dataset about students and exam outcomes. In real projects, you would load data from a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "students = pd.DataFrame(\n",
        "    {\n",
        "        \"study_hours\": [1, 2, 3, 4, 5, 2, 3, 4, 6, 7, 1, 5, 8, 2, 6],\n",
        "        \"practice_tests\": [0, 1, 1, 2, 2, 0, 1, 2, 3, 3, 0, 2, 4, 1, 3],\n",
        "        \"passed\": [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "students.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Split into train and test\n",
        "We need a **hold-out test set** to evaluate generalization. We'll use 80% for training and 20% for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X = students[[\"study_hours\", \"practice_tests\"]]\n",
        "y = students[\"passed\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Train a baseline model\n",
        "Logistic regression is a strong baseline for binary classification.\n",
        "We use a scaler because features are on different scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = Pipeline(\n",
        "    [\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"logreg\", LogisticRegression()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Evaluate with standard metrics\n",
        "A single metric can hide important behavior. We’ll look at multiple metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "preds = model.predict(X_test)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(\"Precision:\", precision_score(y_test, preds))\n",
        "print(\"Recall:\", recall_score(y_test, preds))\n",
        "print(\"F1:\", f1_score(y_test, preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, probs))\n",
        "\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, preds))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Threshold tuning\n",
        "By default, logistic regression uses a 0.5 probability threshold. You can change it to trade off precision and recall.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "threshold = 0.4\n",
        "custom_preds = (probs >= threshold).astype(int)\n",
        "\n",
        "print(\"Custom threshold:\", threshold)\n",
        "print(\"Precision:\", precision_score(y_test, custom_preds))\n",
        "print(\"Recall:\", recall_score(y_test, custom_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) What to do next\n",
        "Now that you have a baseline, the next steps are:\n",
        "- Try a more flexible model (Decision Trees in Day 03)\n",
        "- Compare metrics to ensure real improvement\n",
        "- Keep the baseline as a reference\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}