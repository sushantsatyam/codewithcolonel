{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 06 — Interpretability Basics\n",
        "\n",
        "Interpretable models help you **trust** and **debug** predictions.\n",
        "\n",
        "We will cover:\n",
        "- Global feature importance\n",
        "- Permutation importance\n",
        "- Partial dependence plots\n",
        "- Inspecting a single prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Train a model\n",
        "We’ll use a random forest on the breast cancer dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "accuracy_score(y_test, rf.predict(X_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Built-in feature importance (global)\n",
        "Tree models expose a basic importance score based on splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "importances.sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Permutation importance (global)\n",
        "Permutation importance measures how much performance drops when a feature is shuffled.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "perm = permutation_importance(rf, X_test, y_test, n_repeats=5, random_state=42)\n",
        "perm_importances = pd.Series(perm.importances_mean, index=X.columns)\n",
        "perm_importances.sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Partial dependence plots (global)\n",
        "Partial dependence shows how a feature affects predictions on average.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "features = [\"mean radius\", \"mean texture\"]\n",
        "PartialDependenceDisplay.from_estimator(rf, X_test, features)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Inspect a single prediction (local)\n",
        "We can look at the model’s predicted probability for one row.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "row = X_test.iloc[[0]]\n",
        "proba = rf.predict_proba(row)[0, 1]\n",
        "\n",
        "row.T.head(8), proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) What to do next\n",
        "Interpretability is a deep topic. Next steps:\n",
        "- Try SHAP for local explanations\n",
        "- Compare explanations across different models\n",
        "- Use explanations to debug unexpected predictions\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}