{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "id": "md-01",
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 01 \u2014 Data Cleaning + EDA Fundamentals\n\nWelcome to **Day 01 of the ML Track!** This is a comprehensive walkthrough for **Exploratory Data Analysis (EDA)** \u2014 the essential first step in every ML project.\n\n## Why EDA is Non-Negotiable\n\n| Reason | What happens if you skip it |\n|---|---|\n| **Data quality** | Garbage in, garbage out. Missing values or wrong types silently corrupt your model. |\n| **Feature understanding** | You miss obvious transformations that would boost performance. |\n| **Target leakage** | You accidentally include future information, getting unrealistically high scores. |\n| **Model selection** | You pick the wrong model type because you never looked at the distributions. |\n\n## The EDA Workflow\n\n```\nRaw Data -> 1. Inspect -> 2. Missing Values -> 3. Summary Stats\n         -> 4. Distributions -> 5. Correlation -> 6. Feature Engineering\n         -> Clean Data ready for Modeling (Day 02)\n```\n\n## What We Cover Today\n\n1. **Theory**: Types of data (numerical, categorical, ordinal)\n2. **Theory**: Summary statistics \u2014 mean, median, std, quartiles\n3. **Theory**: Missing values \u2014 MCAR/MAR/MNAR and imputation strategies\n4. **Theory**: Distributions, skewness, outlier detection\n5. **Theory**: Correlation \u2014 Pearson r, heatmaps, caveats\n6. **Theory**: Feature engineering \u2014 ratios and bucketing\n7. **Practice**: All of the above on a small HR dataset"
  },
  {
   "id": "md-02",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Types of Data\n\n### Numerical \u2014 quantities you can do arithmetic on\n\n| Subtype | Description | Examples |\n|---|---|---|\n| **Continuous** | Any value within a range | Age (22.5), salary, temperature |\n| **Discrete** | Only specific values (usually integers) | Number of children, floor number |\n\n### Categorical \u2014 groups or labels (arithmetic is meaningless)\n\n| Subtype | Description | Examples |\n|---|---|---|\n| **Nominal** | No natural order | Department (sales, marketing), country |\n| **Ordinal** | Meaningful order | Education level, star ratings (1-5) |\n\n### Why This Matters for ML\n\n| Data Type | Summary Stats | Visualizations | Model Encoding |\n|---|---|---|---|\n| **Continuous** | Mean, median, std | Histogram, boxplot | Use directly (maybe scale) |\n| **Nominal** | Mode, value_counts | Bar chart | One-hot encoding |\n| **Ordinal** | Median, mode | Bar chart | Label encoding (preserves order) |\n\nPandas does not automatically know the correct type \u2014 it is your job to set it."
  },
  {
   "id": "md-03",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Setup and Imports"
  },
  {
   "id": "code-imports",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (8, 5)\nplt.rcParams['figure.dpi'] = 100\npd.set_option('display.max_columns', 20)\n\nprint('All imports successful!')"
  },
  {
   "id": "md-04",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Creating a Sample Dataset\n\nIn real projects you load a CSV or SQL table. Here we create a small in-memory dataset\nto focus on EDA concepts. It intentionally has **missing values** and mixed **data types**."
  },
  {
   "id": "code-dataset",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "data = {\n    'age':        [22, 35, 28, None, 40, 19, 50],\n    'salary':     [48000, 54000, 50000, 62000, None, 41000, 80000],\n    'department': ['sales', 'marketing', 'sales', 'engineering',\n                   'engineering', 'sales', 'marketing'],\n    'tenure':     [1.2, 3.4, 2.1, 5.0, 4.2, 0.8, 6.5],\n}\ndf = pd.DataFrame(data)\nprint(f'Shape: {df.shape[0]} rows x {df.shape[1]} columns')\ndf.head()"
  },
  {
   "id": "md-05",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: The First Three Questions\n\n1. **How big is the data?** -> `df.shape`\n2. **What types do I have?** -> `df.info()` (shows dtypes and non-null counts)\n3. **What does it look like?** -> `df.head()` / `df.tail()`\n\n### Pandas Data Types\n\n| dtype | What it represents |\n|---|---|\n| `int64` | Integers (no decimals) |\n| `float64` | Decimals \u2014 **also used when integers have missing values** |\n| `object` | Text strings or mixed types |\n| `category` | Categorical (memory-efficient; correct for nominals/ordinals) |\n\n**Important:** When an integer column has `None`, pandas converts it to `float64`\nbecause `NaN` is a float. You will see this in the `age` column."
  },
  {
   "id": "code-inspect",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=' * 50)\nprint('DATASET SIZE')\nprint('=' * 50)\nprint(f'Rows:    {df.shape[0]}')\nprint(f'Columns: {df.shape[1]}')\nprint(f'Names:   {list(df.columns)}')\nprint()\n\nprint('=' * 50)\nprint('DATA TYPES AND NON-NULL COUNTS')\nprint('=' * 50)\ndf.info()\nprint()\nprint('OBSERVATIONS:')\nprint('- age is float64 (not int64) because it has a missing value (NaN is a float)')\nprint('- salary is float64 for the same reason')\nprint('- age and salary each have 6/7 non-null => 1 missing value each')"
  },
  {
   "id": "md-06",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Summary Statistics\n\n### Central Tendency\n\n| Statistic | What it tells you | Sensitive to outliers? |\n|---|---|---|\n| **Mean** | Center of gravity of the data | **Yes** \u2014 one extreme value pulls it strongly |\n| **Median** | Typical value \u2014 50% above, 50% below | **No** \u2014 robust to outliers |\n| **Mode** | Most frequent value | No |\n\n### Spread\n\n| Statistic | What it tells you |\n|---|---|\n| **Std** | Average distance of values from the mean |\n| **IQR** | Q3 - Q1 \u2014 spread of the middle 50% (robust to outliers) |\n\n### Worked Example\n\nAge values (excluding the missing one): 19, 22, 28, 35, 40, 50\n\n- **Mean** = (19+22+28+35+40+50)/6 = **32.3**\n- **Median** = average of 28 and 35 = **31.5**\n- mean (32.3) > median (31.5) => slight **right skew**\n\n**Rule:** mean > median = right-skewed | mean < median = left-skewed | mean \u2248 median = symmetric"
  },
  {
   "id": "code-stats",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# .describe() gives count, mean, std, min, 25%, 50%, 75%, max for every numeric column\nprint('NUMERICAL SUMMARY STATISTICS')\nprint(df.describe().round(2))\nprint()\nprint('HOW TO READ THIS:')\nprint('- count < total rows => missing values')\nprint('- mean vs 50% (median): large gap = skewed distribution')\nprint('- min/max: check for impossible values (negative age? salary=0?)')\nprint()\n\n# For categorical columns, value_counts() is the equivalent of describe()\nprint('CATEGORICAL: department')\nprint(df['department'].value_counts())\nprint()\nfor dept, pct in (df['department'].value_counts(normalize=True)*100).items():\n    print(f'  {dept}: {pct:.1f}%')"
  },
  {
   "id": "md-07",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Missing Values \u2014 Types and Strategies\n\n### Three Types of Missingness\n\n| Type | Description | Implication |\n|---|---|---|\n| **MCAR** (Missing Completely At Random) | No relationship to any variable | Safe to drop or impute |\n| **MAR** (Missing At Random) | Depends on observed variables, not the missing value | Imputation using other features works well |\n| **MNAR** (Missing Not At Random) | Depends on the missing value itself | Any strategy may introduce bias |\n\n**Example of MNAR:** High-income people refuse to report salary. Imputing with the mean\nwill systematically underestimate their salary.\n\n### Strategies\n\n| Strategy | When to use |\n|---|---|\n| **Drop rows** | Very few missing (<5%), MCAR |\n| **Median imputation** | Continuous, skewed or with outliers \u2014 more robust than mean |\n| **Mean imputation** | Continuous, roughly symmetric |\n| **Mode imputation** | Categorical |\n| **Indicator column** | When missingness itself carries signal (add BEFORE imputing) |\n\n> Always **quantify first** before deciding on a strategy."
  },
  {
   "id": "code-missing",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Quantify\nprint('MISSING VALUE ANALYSIS')\nmissing = pd.DataFrame({'count': df.isna().sum(), 'pct': (df.isna().sum()/len(df)*100).round(1)})\nprint(missing)\nprint()\n\n# Step 2: Add indicator column BEFORE imputing\n# Records WHERE data was originally missing \u2014 preserves missingness signal\ndf['age_missing'] = df['age'].isna().astype(int)\nprint('age_missing indicator (1=was missing):')\nprint(df[['age', 'age_missing']].to_string())\nprint()\n\n# Step 3: Impute with median (robust to outliers)\nage_med = df['age'].median()\nsal_med = df['salary'].median()\nprint(f'Imputing age with median:    {age_med}')\nprint(f'Imputing salary with median: {sal_med}')\ndf['age'] = df['age'].fillna(age_med)\ndf['salary'] = df['salary'].fillna(sal_med)\nprint()\nprint(f'Missing after imputation: {df.isna().sum().sum()} values')"
  },
  {
   "id": "md-08",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Data Types and Casting\n\n### Why Cast Dtypes Explicitly?\n\n1. **Memory efficiency**: `category` uses 95%+ less memory than `object` for columns\n   with few unique values.\n2. **Correct operations**: `.describe()` shows mean/std for numeric; count/unique for\n   object/category.\n3. **Prevents bugs**: A numeric column stored as `object` silently fails arithmetic.\n\n| From | To | Method |\n|---|---|---|\n| `object` | `category` | `.astype('category')` |\n| `object` | `float64` | `pd.to_numeric(col, errors='coerce')` |\n| `object` | `datetime64` | `pd.to_datetime(col)` |"
  },
  {
   "id": "code-dtypes",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f'Before: department dtype = {df[\"department\"].dtype}')\ndf['department'] = df['department'].astype('category')\nprint(f'After:  department dtype = {df[\"department\"].dtype}')\nprint(f'Categories: {list(df[\"department\"].cat.categories)}')\nprint()\nprint('FINAL COLUMN TYPES')\nprint(df.dtypes)"
  },
  {
   "id": "md-09",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Distributions, Skewness, and Outliers\n\n### Distribution Shapes\n\n```\nSymmetric         Right-Skewed      Left-Skewed\n    @                 @                  @\n   @@@               @@@               @@@  \n  @@@@@             @@@@@@           @@@@@@\n @@@@@@@@         @@@@@@@@@@       @@@@@@@@@@\n---------       -------------    -------------\nmean=median     mean > median     mean < median\n```\n\n### The 1.5 x IQR Outlier Rule (Boxplot Whiskers)\n\n- Lower fence: Q1 - 1.5 x IQR\n- Upper fence: Q3 + 1.5 x IQR\n- Values beyond fences are **potential outliers** (shown as dots in boxplots)\n\n**Example:** Q1=25, Q3=45, IQR=20 => fences at -5 and 75.\n\n### KDE (Kernel Density Estimation)\nA smooth version of a histogram. Use `kde=True` in seaborn's `histplot`."
  },
  {
   "id": "code-dist",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Histograms with KDE overlay and mean/median reference lines\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, col, color in zip(axes, ['age', 'salary'], ['steelblue', 'coral']):\n    sns.histplot(df[col], kde=True, color=color, edgecolor='white', ax=ax)\n    m, med = df[col].mean(), df[col].median()\n    ax.axvline(m, color='red', linestyle='--', lw=1.5, label=f'Mean={m:.1f}')\n    ax.axvline(med, color='green', linestyle='-.', lw=1.5, label=f'Median={med:.1f}')\n    ax.set_title(f'{col.capitalize()} Distribution', fontsize=13, fontweight='bold')\n    ax.legend(fontsize=9)\n\nplt.suptitle('Distribution Analysis', fontsize=15, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Boxplot: Salary by department\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.boxplot(x='department', y='salary', data=df, palette='Set2', ax=ax)\nsns.stripplot(x='department', y='salary', data=df, color='black', size=8, alpha=0.7, ax=ax)\nax.set_title('Salary by Department', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Salary by department:')\nfor dept in df['department'].cat.categories:\n    s = df[df['department']==dept]['salary']\n    print(f'  {dept}: median=${s.median():,.0f}, range=${s.min():,.0f}-${s.max():,.0f}')"
  },
  {
   "id": "md-10",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Correlation and Relationships\n\n### Pearson r \u2014 Measures Linear Relationship Strength\n\n| r value | Interpretation |\n|---|---|\n| +1.0 | Perfect positive linear relationship |\n| +0.7 to +0.9 | Strong positive correlation |\n| 0 | No linear relationship |\n| -0.7 to -1.0 | Strong negative correlation |\n\n### Critical Caveats\n\n1. **Correlation does not imply causation.** Ice cream sales and drowning rates both\n   peak in summer \u2014 ice cream does not cause drowning.\n\n2. **Linear only.** A perfect parabola (y = x^2) can have r = 0.\n\n3. **Sensitive to outliers.** One extreme point can drastically change r."
  },
  {
   "id": "code-corr",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "corr = df.select_dtypes(include='number').corr()\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n            vmin=-1, vmax=1, square=True, linewidths=0.5, ax=ax)\nax.set_title('Correlation Matrix (Pearson)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Notable correlations (|r| > 0.4):')\ncols = corr.columns\nfound = False\nfor i in range(len(cols)):\n    for j in range(i+1, len(cols)):\n        r = corr.iloc[i, j]\n        if abs(r) > 0.4:\n            found = True\n            s = 'strong' if abs(r) > 0.7 else 'moderate'\n            d = 'positive' if r > 0 else 'negative'\n            print(f'  {cols[i]} vs {cols[j]}: r={r:.2f} ({s} {d})')\nif not found:\n    print('  None found.')"
  },
  {
   "id": "md-11",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Theory: Feature Engineering \u2014 Ratios and Bucketing\n\nCreating new features from existing ones is often the single biggest lever for improving\nmodel performance.\n\n### Why Create New Features?\n\n- A burnout risk model cares about **salary relative to tenure**, not each value alone.\n- A career prediction model may care about **age group** more than exact age.\n\n### Common Techniques\n\n| Technique | Example | When to Use |\n|---|---|---|\n| **Ratio** | salary / tenure | When the ratio carries more signal than each value separately |\n| **Bucketing** | age -> '<25', '25-35' | When the effect is non-linear across the value range |\n| **Log transform** | log(salary) | When a feature is heavily right-skewed |\n\n### Warning: Avoid Target Leakage\n\nNever create features using information unavailable at prediction time."
  },
  {
   "id": "code-features",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature 1: Ratio\n# salary_per_year = pay per year of experience\n# +0.1 prevents division by zero for very new hires\ndf['salary_per_year'] = df['salary'] / (df['tenure'] + 0.1)\n\n# Feature 2: Bucketing\n# age_bucket = career stage categories\ndf['age_bucket'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 100],\n                          labels=['<25', '25-35', '35-45', '45+'])\n\nprint('New features:')\nprint(df[['age', 'salary', 'tenure', 'salary_per_year', 'age_bucket']].to_string())\nprint()\nprint(f'salary_per_year: min=${df[\"salary_per_year\"].min():,.0f}, max=${df[\"salary_per_year\"].max():,.0f}')\nprint()\nprint('age_bucket distribution:')\nprint(df['age_bucket'].value_counts().sort_index())"
  },
  {
   "id": "md-12",
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary of Findings\n\n### Theory Recap\n\n| Topic | Key Takeaway |\n|---|---|\n| **Data types** | Numerical vs categorical. Each needs different summary stats, visualizations, and encoding. |\n| **Summary statistics** | Mean is sensitive to outliers; median is robust. Compare them to detect skew. |\n| **Missing values** | Understand MCAR/MAR/MNAR. Add indicator columns before imputing. |\n| **Distributions** | Histograms reveal shape; boxplots show quartiles and flag outliers. |\n| **Correlation** | Pearson r is linear only. Correlation does not imply causation. |\n| **Feature engineering** | Ratios and buckets create more informative signals than raw columns. |\n\n### Practice Completed\n\n1. Inspected structure: `df.shape`, `df.info()`, `df.head()`\n2. Quantified missing values, added indicator column, imputed with median\n3. Computed summary statistics with `df.describe()` and `value_counts()`\n4. Cast `department` to `category` dtype\n5. Visualized distributions: histograms with KDE and mean/median lines, boxplots by group\n6. Built Pearson correlation heatmap\n7. Engineered `salary_per_year` (ratio) and `age_bucket` (bucketing)\n\n> EDA is not optional. It is the foundation of every reliable ML pipeline.\n\n---\n## Next Steps: Day 02 \u2014 Baseline Model\n\nIn **Day 02**, we build a **logistic regression baseline** on the Breast Cancer Wisconsin dataset.\nToday's work connects directly:\n\n- The **imputation** is required before passing data to any sklearn model (no NaN allowed).\n- The **dtype casting** determines how we encode features (one-hot vs scaling).\n- The **correlation analysis** helps identify useful vs redundant predictors.\n- The **feature engineering** ideas can be applied before modeling for better inputs.\n\n**See you in Day 02!**"
  }
 ]
}