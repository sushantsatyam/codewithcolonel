{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "id": "md-title",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 01 \u2014 Data Cleaning + EDA Fundamentals\n",
    "\n",
    "Welcome to **Day 01 of the ML Track!** This notebook is a comprehensive, beginner-friendly\n",
    "walkthrough for **Exploratory Data Analysis (EDA)** \u2014 the essential first step in every\n",
    "machine learning project.\n",
    "\n",
    "## What Is EDA and Why Does It Matter?\n",
    "\n",
    "**Exploratory Data Analysis** is the process of examining a dataset to understand its structure,\n",
    "spot problems, and develop intuitions \u2014 all *before* building any models.\n",
    "\n",
    "### Why EDA is non-negotiable\n",
    "\n",
    "| Reason | What happens if you skip it |\n",
    "|---|---|\n",
    "| **Data quality** | Garbage in, garbage out. Undetected missing values or wrong types corrupt your model. |\n",
    "| **Feature understanding** | You build features blindly, missing obvious transformations. |\n",
    "| **Target leakage** | You accidentally include future information as a feature. |\n",
    "| **Model selection** | You pick the wrong model because you never looked at the distributions. |\n",
    "\n",
    "### The EDA Workflow\n",
    "\n",
    "\n",
    "\n",
    "## What We Will Cover Today\n",
    "\n",
    "1. **Theory**: Types of data (numerical, categorical, ordinal)\n",
    "2. **Theory**: Summary statistics \u2014 mean, median, std, quartiles\n",
    "3. **Theory**: Missing values \u2014 MCAR/MAR/MNAR and imputation strategies\n",
    "4. **Theory**: Distributions, skewness, and outlier detection\n",
    "5. **Theory**: Correlation and relationships between features\n",
    "6. **Theory**: Feature engineering \u2014 ratios and bucketing\n",
    "7. **Practice**: All of the above on a small HR dataset\n",
    "\n",
    "**Goal:** By the end, you should feel comfortable exploring any new dataset before modeling."
   ]
  },
  {
   "id": "md-types",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Types of Data\n",
    "\n",
    "Every column in a dataset falls into one of these categories:\n",
    "\n",
    "### Numerical Data \u2014 represents quantities you can do arithmetic on\n",
    "\n",
    "| Subtype | Description | Examples |\n",
    "|---|---|---|\n",
    "| **Continuous** | Any value within a range | Age (22.5), salary (8,123), temperature |\n",
    "| **Discrete** | Only specific values (usually integers) | Number of children (0,1,2), floor number |\n",
    "\n",
    "### Categorical Data \u2014 represents groups or labels\n",
    "\n",
    "| Subtype | Description | Examples |\n",
    "|---|---|---|\n",
    "| **Nominal** | No natural order | Department (sales, marketing), color, country |\n",
    "| **Ordinal** | Meaningful order | Education level, star ratings (1-5) |\n",
    "\n",
    "### Why This Matters for ML\n",
    "\n",
    "| Data Type | Summary Stats | Visualizations | Model Encoding |\n",
    "|---|---|---|---|\n",
    "| **Continuous** | Mean, median, std | Histogram, boxplot | Use directly (maybe scale) |\n",
    "| **Nominal** | Mode, value_counts | Bar chart | One-hot encoding |\n",
    "| **Ordinal** | Median, mode | Bar chart | Label encoding (preserves order) |\n",
    "\n",
    "**Key insight:** Pandas does not automatically know the correct type. It is your job\n",
    "to understand and handle each column appropriately."
   ]
  },
  {
   "id": "md-setup",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "id": "code-imports",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd    # DataFrames for tabular data\n",
    "import numpy as np     # Numerical operations\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots appear inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "\n",
    "print(\"All imports successful. Ready to explore!\")"
   ]
  },
  {
   "id": "md-dataset",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a Sample Dataset\n",
    "\n",
    "In real projects you would load a CSV, SQL table, or API response. Here we create a\n",
    "small in-memory dataset to keep the focus on **EDA concepts** rather than data loading.\n",
    "\n",
    "Our dataset simulates an HR table with employee information. It intentionally includes\n",
    "**missing values** and a mix of **data types** \u2014 just like real-world data."
   ]
  },
  {
   "id": "code-dataset",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small employee dataset with realistic imperfections:\n",
    "#   - Missing values (None) in age and salary\n",
    "#   - Mix of data types: numeric (age, salary, tenure), categorical (department)\n",
    "#   - Very different scales: age is 19-50, salary is 41000-80000\n",
    "data = {\n",
    "    \"age\":        [22, 35, 28, None, 40, 19, 50],\n",
    "    \"salary\":     [48000, 54000, 50000, 62000, None, 41000, 80000],\n",
    "    \"department\": [\"sales\", \"marketing\", \"sales\", \"engineering\",\n",
    "                   \"engineering\", \"sales\", \"marketing\"],\n",
    "    \"tenure\":     [1.2, 3.4, 2.1, 5.0, 4.2, 0.8, 6.5],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print()\n",
    "df.head()"
   ]
  },
  {
   "id": "md-inspect-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: The First Three Questions\n",
    "\n",
    "When you encounter a new dataset, always start with these three questions:\n",
    "\n",
    "1. **How big is the data?** ->  gives (rows, columns)\n",
    "2. **What types of data do I have?** ->  shows dtypes and non-null counts\n",
    "3. **What does it look like?** ->  /  give a visual sanity check\n",
    "\n",
    "### Understanding Pandas Data Types\n",
    "\n",
    "| Pandas dtype | What it represents |\n",
    "|---|---|\n",
    "|  | Integer numbers (no decimals) |\n",
    "|  | Decimal numbers \u2014 **also used when integers have missing values** |\n",
    "|  | Text strings or mixed types |\n",
    "|  | Categorical data (memory-efficient; correct for nominals/ordinals) |\n",
    "|  | Dates and timestamps |\n",
    "\n",
    "**Important:** When a column of integers contains , pandas converts it to \n",
    "because  (Not a Number) is a float. You will see this in our  column."
   ]
  },
  {
   "id": "code-inspect",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Question 1: How big is the data? ----\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET SIZE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rows:    {df.shape[0]}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"Names:   {list(df.columns)}\")\n",
    "print()\n",
    "\n",
    "# ---- Question 2: What data types? ----\n",
    "# .info() shows column names, non-null counts, and dtypes all at once.\n",
    "# It is the single most useful method for initial dataset inspection.\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA TYPES AND NON-NULL COUNTS\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "print()\n",
    "\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"- age is float64 (not int64) because it has a missing value\")\n",
    "print(\"- salary is float64 for the same reason\")\n",
    "print(\"- age has 6 non-null out of 7 => 1 missing value\")\n",
    "print(\"- salary has 6 non-null out of 7 => 1 missing value\")"
   ]
  },
  {
   "id": "md-stats-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Summary Statistics\n",
    "\n",
    "### Measures of Central Tendency\n",
    "\n",
    "| Statistic | What it tells you | Sensitive to outliers? |\n",
    "|---|---|---|\n",
    "| **Mean** | Center of gravity of the data | **Yes** \u2014 one extreme value pulls it strongly |\n",
    "| **Median** | Typical value \u2014 50% above, 50% below | **No** \u2014 robust to outliers |\n",
    "| **Mode** | Most frequent value (useful for categorical) | No |\n",
    "\n",
    "### Measures of Spread\n",
    "\n",
    "| Statistic | What it tells you |\n",
    "|---|---|\n",
    "| **Std** | Average distance of values from the mean |\n",
    "| **Range** | max - min (sensitive to outliers) |\n",
    "| **IQR** | Q3 - Q1 \u2014 spread of the middle 50% (robust to outliers) |\n",
    "\n",
    "### Worked Example\n",
    "\n",
    "Age values (after removing the missing one): 19, 22, 28, 35, 40, 50\n",
    "\n",
    "- **Mean** = (19+22+28+35+40+50)/6 = **32.3**\n",
    "- **Median** = average of 28 and 35 = **31.5** (middle two values, n=6 is even)\n",
    "- Here mean (32.3) > median (31.5) => slight **right skew**\n",
    "\n",
    "**Rule of thumb:**\n",
    "- mean \u2248 median => symmetric distribution\n",
    "- mean > median => right-skewed (long tail to the right)\n",
    "- mean < median => left-skewed (long tail to the left)"
   ]
  },
  {
   "id": "code-stats",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() computes count, mean, std, min, 25%, 50%, 75%, max\n",
    "# for every numeric column. This is your EDA best friend.\n",
    "print(\"=\" * 55)\n",
    "print(\"NUMERICAL SUMMARY STATISTICS\")\n",
    "print(\"=\" * 55)\n",
    "print(df.describe().round(2))\n",
    "print()\n",
    "print(\"HOW TO READ THIS:\")\n",
    "print(\"- count < total rows => missing values\")\n",
    "print(\"- mean vs 50% (median): large gap = skewed distribution\")\n",
    "print(\"- std large relative to mean = high variability\")\n",
    "print(\"- min/max: check for impossible values (negative age? salary=0?)\")\n",
    "print()\n",
    "\n",
    "# For categorical columns, value_counts() is the equivalent of describe()\n",
    "print(\"=\" * 55)\n",
    "print(\"CATEGORICAL SUMMARY: department\")\n",
    "print(\"=\" * 55)\n",
    "print(df[\"department\"].value_counts())\n",
    "print()\n",
    "dept_pct = df[\"department\"].value_counts(normalize=True) * 100\n",
    "for dept, pct in dept_pct.items():\n",
    "    print(f\"  {dept}: {pct:.1f}%\")"
   ]
  },
  {
   "id": "md-missing-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Missing Values \u2014 Types and Strategies\n",
    "\n",
    "### Three Types of Missingness\n",
    "\n",
    "| Type | Description | Implication |\n",
    "|---|---|---|\n",
    "| **MCAR** (Missing Completely At Random) | No relationship to any variable | Safe to drop or impute |\n",
    "| **MAR** (Missing At Random) | Depends on observed variables, not the missing value | Imputation using other features works well |\n",
    "| **MNAR** (Missing Not At Random) | Depends on the missing value itself | Any strategy may introduce bias |\n",
    "\n",
    "**Example of MNAR:** High-income people refuse to report salary. If you impute with the mean,\n",
    "you are systematically underestimating salary for those people.\n",
    "\n",
    "### Common Strategies\n",
    "\n",
    "| Strategy | When to use | Risk |\n",
    "|---|---|---|\n",
    "| **Drop rows** | Very few missing (<5%), MCAR | Loses data |\n",
    "| **Mean imputation** | Continuous, symmetric distribution | Underestimates variance |\n",
    "| **Median imputation** | Continuous, skewed or with outliers | Same as mean, more robust |\n",
    "| **Mode imputation** | Categorical | Over-represents dominant category |\n",
    "| **Indicator column** | When missingness itself is a signal | More features |\n",
    "\n",
    "> Always **quantify first** \u2014 understand the scale of the problem before deciding."
   ]
  },
  {
   "id": "code-missing",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Quantify ----\n",
    "print(\"=\" * 50)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "missing = pd.DataFrame({\n",
    "    \"count\": df.isna().sum(),\n",
    "    \"pct\": (df.isna().sum() / len(df) * 100).round(1)\n",
    "})\n",
    "print(missing)\n",
    "print()\n",
    "\n",
    "# ---- Step 2: Add indicator column BEFORE imputing ----\n",
    "# Records WHERE data was originally missing.\n",
    "# Preserves missingness signal in case the data is MNAR.\n",
    "df[\"age_missing\"] = df[\"age\"].isna().astype(int)\n",
    "print(\"Added age_missing indicator (1=was missing, 0=was present):\")\n",
    "print(df[[\"age\", \"age_missing\"]].to_string())\n",
    "print()\n",
    "\n",
    "# ---- Step 3: Impute with median ----\n",
    "# Median is preferred over mean because it is robust to outliers.\n",
    "age_median = df[\"age\"].median()\n",
    "salary_median = df[\"salary\"].median()\n",
    "print(f\"Imputing age with median:    {age_median}\")\n",
    "print(f\"Imputing salary with median: {salary_median}\")\n",
    "\n",
    "df[\"age\"] = df[\"age\"].fillna(age_median)\n",
    "df[\"salary\"] = df[\"salary\"].fillna(salary_median)\n",
    "\n",
    "print()\n",
    "print(\"Missing after imputation:\", df.isna().sum().sum(), \"values\")\n",
    "print(\"RESULT: All missing values handled.\")"
   ]
  },
  {
   "id": "md-dtypes-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Data Types and Casting\n",
    "\n",
    "### Why Explicitly Cast Dtypes?\n",
    "\n",
    "1. **Memory efficiency**:  dtype uses 95%+ less memory than  for\n",
    "   columns with few unique values (like department with 3 values).\n",
    "\n",
    "2. **Correct operations**:  shows mean/std for numeric columns but\n",
    "   count/unique/top/freq for object/category columns.\n",
    "\n",
    "3. **Preventing bugs**: A numeric column accidentally stored as  will silently\n",
    "   fail arithmetic operations.\n",
    "\n",
    "### Common Casts\n",
    "\n",
    "| From | To | Method |\n",
    "|---|---|---|\n",
    "|  |  |  |\n",
    "|  |  |  |\n",
    "|  |  |  |"
   ]
  },
  {
   "id": "code-dtypes",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before: department dtype = {df['department'].dtype}\")\n",
    "\n",
    "# Cast to category: more memory-efficient and semantically correct\n",
    "df[\"department\"] = df[\"department\"].astype(\"category\")\n",
    "\n",
    "print(f\"After:  department dtype = {df['department'].dtype}\")\n",
    "print(f\"Categories: {list(df['department'].cat.categories)}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(\"FINAL COLUMN TYPES\")\n",
    "print(\"=\" * 45)\n",
    "print(df.dtypes)\n",
    "print()\n",
    "print(\"All dtypes correct:\")\n",
    "print(\"- age, salary, tenure: float64 (numeric)\")\n",
    "print(\"- department: category (nominal categorical)\")\n",
    "print(\"- age_missing: int64 (binary indicator)\")"
   ]
  },
  {
   "id": "md-dist-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Distributions, Skewness, and Outliers\n",
    "\n",
    "Numbers (mean, std) can hide patterns that plots reveal instantly.\n",
    "\n",
    "### Distribution Shapes\n",
    "\n",
    "\n",
    "\n",
    "### The 1.5 x IQR Outlier Rule\n",
    "\n",
    "A value is a **potential outlier** if it is:\n",
    "- Below Q1 - 1.5 x IQR, or\n",
    "- Above Q3 + 1.5 x IQR\n",
    "\n",
    "**Example:** Q1=25, Q3=45, IQR=20\n",
    "- Lower fence: 25 - 30 = **-5**\n",
    "- Upper fence: 45 + 30 = **75**\n",
    "- Values outside [-5, 75] are flagged as outliers\n",
    "\n",
    "Boxplots display this rule visually. Points beyond the whiskers are potential outliers.\n",
    "\n",
    "### KDE (Kernel Density Estimation)\n",
    "\n",
    "KDE is a smooth version of a histogram. It places a curve on each data point and sums\n",
    "them up, giving a continuous probability density estimate. Use  in seaborn."
   ]
  },
  {
   "id": "code-distributions",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Histograms with KDE and mean/median reference lines ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Age distribution\n",
    "sns.histplot(df[\"age\"], kde=True, color=\"steelblue\", edgecolor=\"white\", ax=axes[0])\n",
    "axes[0].set_title(\"Age Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Age (years)\")\n",
    "age_mean = df[\"age\"].mean()\n",
    "age_med = df[\"age\"].median()\n",
    "axes[0].axvline(age_mean, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Mean = {age_mean:.1f}\")\n",
    "axes[0].axvline(age_med, color=\"green\", linestyle=\"-.\", linewidth=1.5, label=f\"Median = {age_med:.1f}\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Salary distribution\n",
    "sns.histplot(df[\"salary\"], kde=True, color=\"coral\", edgecolor=\"white\", ax=axes[1])\n",
    "axes[1].set_title(\"Salary Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Salary ($)\")\n",
    "sal_mean = df[\"salary\"].mean()\n",
    "sal_med = df[\"salary\"].median()\n",
    "axes[1].axvline(sal_mean, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Mean = {sal_mean:,.0f}\")\n",
    "axes[1].axvline(sal_med, color=\"green\", linestyle=\"-.\", linewidth=1.5, label=f\"Median = {sal_med:,.0f}\")\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Distribution Analysis\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Age:    mean={age_mean:.1f}, median={age_med:.1f}\")\n",
    "print(f\"Salary: mean={sal_mean:,.0f}, median={sal_med:,.0f}\")\n",
    "print()\n",
    "\n",
    "# ---- Boxplot: Salary by department ----\n",
    "# Great for comparing distributions across groups and spotting outliers.\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(x=\"department\", y=\"salary\", data=df, palette=\"Set2\", ax=ax)\n",
    "# Overlay raw data points so we can see every individual value\n",
    "sns.stripplot(x=\"department\", y=\"salary\", data=df, color=\"black\", size=8, alpha=0.7, ax=ax)\n",
    "ax.set_title(\"Salary by Department\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Department\")\n",
    "ax.set_ylabel(\"Salary ($)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"HOW TO READ A BOXPLOT:\")\n",
    "print(\"- Box spans Q1 (bottom) to Q3 (top) = middle 50% of values\")\n",
    "print(\"- Line inside box = MEDIAN\")\n",
    "print(\"- Whiskers = farthest points within 1.5 x IQR\")\n",
    "print(\"- Dots beyond whiskers = potential OUTLIERS\")\n",
    "print()\n",
    "for dept in df[\"department\"].cat.categories:\n",
    "    s = df[df[\"department\"] == dept][\"salary\"]\n",
    "    print(f\"  {dept}: median=${s.median():,.0f}, range=${s.min():,.0f}-${s.max():,.0f}\")"
   ]
  },
  {
   "id": "md-corr-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Correlation and Relationships\n",
    "\n",
    "### Pearson Correlation Coefficient (r)\n",
    "\n",
    "$$r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\cdot \\sum (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "| r value | Interpretation |\n",
    "|---|---|\n",
    "| +1.0 | Perfect positive linear relationship |\n",
    "| +0.7 to +0.9 | Strong positive correlation |\n",
    "| +0.4 to +0.7 | Moderate positive correlation |\n",
    "| 0 | No linear relationship |\n",
    "| -0.4 to -0.7 | Moderate negative correlation |\n",
    "| -1.0 | Perfect negative linear relationship |\n",
    "\n",
    "### Critical Caveats\n",
    "\n",
    "1. **Correlation does not imply causation.** Ice cream sales and drowning rates\n",
    "   correlate (both peak in summer) \u2014 ice cream does not cause drowning.\n",
    "\n",
    "2. **Linear relationships only.** A perfect parabola (y = x^2) can have r = 0.\n",
    "\n",
    "3. **Sensitive to outliers.** One extreme point can drastically change r."
   ]
  },
  {
   "id": "code-correlation",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .corr() computes Pearson correlation between every pair of numeric columns.\n",
    "# Result: square matrix where cell (i,j) = correlation between column i and j.\n",
    "corr = df.select_dtypes(include=\"number\").corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,     # Show values in each cell\n",
    "    fmt=\".2f\",      # 2 decimal places\n",
    "    cmap=\"RdBu_r\",  # Red=positive, Blue=negative\n",
    "    center=0,       # Neutral at 0\n",
    "    vmin=-1, vmax=1,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(\"Correlation Matrix (Pearson)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- Diagonal is always 1.0 (each variable correlates perfectly with itself)\")\n",
    "print(\"- Off-diagonal: |r| > 0.5 = noteworthy linear relationship\")\n",
    "print()\n",
    "\n",
    "# Flag notable correlations automatically\n",
    "cols = corr.columns\n",
    "found = False\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        r = corr.iloc[i, j]\n",
    "        if abs(r) > 0.4:\n",
    "            found = True\n",
    "            s = \"strong\" if abs(r) > 0.7 else \"moderate\"\n",
    "            d = \"positive\" if r > 0 else \"negative\"\n",
    "            print(f\"  {cols[i]} vs {cols[j]}: r={r:.2f} ({s} {d})\")\n",
    "if not found:\n",
    "    print(\"  No correlations with |r| > 0.4 found.\")"
   ]
  },
  {
   "id": "md-feat-theory",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Theory: Feature Engineering \u2014 Ratios and Bucketing\n",
    "\n",
    "Feature engineering creates **new features** from existing ones. It is often\n",
    "the single biggest lever for improving model performance.\n",
    "\n",
    "### Why Create New Features?\n",
    "\n",
    "Raw features sometimes do not capture what a model needs:\n",
    "- A burnout risk model cares about **salary relative to tenure**, not each alone.\n",
    "- A career prediction model may care about **age group** (junior/senior) more than exact age.\n",
    "\n",
    "### Common Techniques\n",
    "\n",
    "| Technique | Example | When to Use |\n",
    "|---|---|---|\n",
    "| **Ratio** | salary / tenure | When the ratio carries more signal than each value separately |\n",
    "| **Bucketing** | age -> \"<25\", \"25-35\" | When the effect of a variable is non-linear across its range |\n",
    "| **Log transform** | log(salary) | When a feature is heavily right-skewed |\n",
    "| **Interaction** | age x tenure | When combined effect of two features matters |\n",
    "\n",
    "### Warning: Avoid Target Leakage\n",
    "\n",
    "Never create features using information unavailable at prediction time.\n",
    "If predicting next month salary, you cannot use next month performance review \u2014 it does not exist yet."
   ]
  },
  {
   "id": "code-features",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Feature 1: Ratio ----\n",
    "# salary_per_year: pay per year of experience\n",
    "# +0.1 avoids division by zero for brand-new hires with tenure near 0\n",
    "df[\"salary_per_year\"] = df[\"salary\"] / (df[\"tenure\"] + 0.1)\n",
    "\n",
    "# ---- Feature 2: Bucketing ----\n",
    "# age_bucket: group employees into career stage categories\n",
    "# pd.cut() converts continuous values into labeled bins\n",
    "df[\"age_bucket\"] = pd.cut(\n",
    "    df[\"age\"],\n",
    "    bins=[0, 25, 35, 45, 100],\n",
    "    labels=[\"<25\", \"25-35\", \"35-45\", \"45+\"]\n",
    ")\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"NEW FEATURES\")\n",
    "print(\"=\" * 65)\n",
    "print(df[[\"age\", \"salary\", \"tenure\", \"salary_per_year\", \"age_bucket\"]].to_string())\n",
    "print()\n",
    "print(\"salary_per_year summary:\")\n",
    "print(f\"  Min:  ${df['salary_per_year'].min():,.0f}\")\n",
    "print(f\"  Max:  ${df['salary_per_year'].max():,.0f}\")\n",
    "print(f\"  Mean: ${df['salary_per_year'].mean():,.0f}\")\n",
    "print()\n",
    "print(\"age_bucket distribution:\")\n",
    "print(df[\"age_bucket\"].value_counts().sort_index())"
   ]
  },
  {
   "id": "md-summary",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary of Findings\n",
    "\n",
    "### Theory Recap\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|---|---|\n",
    "| **Data types** | Numerical (continuous/discrete) vs categorical (nominal/ordinal). Each needs different handling. |\n",
    "| **Summary statistics** | Mean is sensitive to outliers; median is robust. Compare them to detect skew. |\n",
    "| **Missing values** | Understand MCAR/MAR/MNAR. Add indicator columns before imputing. |\n",
    "| **Distributions** | Histograms reveal shape; boxplots reveal quartiles and outliers. |\n",
    "| **Correlation** | Pearson r measures linear relationships only. Correlation != causation. |\n",
    "| **Feature engineering** | Ratios and buckets can create more informative signals than raw columns. |\n",
    "\n",
    "### Practice Completed\n",
    "\n",
    "1. **Inspected** structure: , , \n",
    "2. **Quantified** missing values and applied median imputation with indicator column\n",
    "3. **Computed** summary statistics:  and \n",
    "4. **Cast** department to  dtype\n",
    "5. **Visualized** distributions: histograms with KDE + mean/median lines, boxplots by group\n",
    "6. **Built** Pearson correlation heatmap\n",
    "7. **Engineered**  (ratio) and  (binning)\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "> EDA is not optional. It is the foundation of every reliable ML pipeline.\n",
    "> A model trained on poorly understood data is a model waiting to fail in production.\n",
    "\n",
    "---\n",
    "## Next Steps: Connection to Day 02\n",
    "\n",
    "In **Day 02**, we build our first model \u2014 a **logistic regression baseline** on\n",
    "the Breast Cancer Wisconsin dataset. Today's work connects directly:\n",
    "\n",
    "- The **imputation** we did is required before passing data to any sklearn model\n",
    "  (it does not handle NaN by default).\n",
    "- The **dtype casting** influences how we encode features (one-hot for categoricals,\n",
    "  scaling for numerics).\n",
    "- The **correlation analysis** helps decide which features are likely useful predictors.\n",
    "- The **feature engineering** ideas can be applied before modeling for better inputs.\n",
    "\n",
    "**See you in Day 02!**"
   ]
  }
 ]
}