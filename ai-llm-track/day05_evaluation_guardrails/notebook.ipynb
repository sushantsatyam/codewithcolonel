{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 05 — Evaluation + guardrails\n",
        "\n",
        "Evaluation helps you check if the model meets quality and safety requirements.\n",
        "\n",
        "We will cover:\n",
        "- Creating a rubric\n",
        "- Scoring outputs\n",
        "- Adding guardrail checks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Define a simple rubric\n",
        "We’ll score responses on clarity, completeness, and safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "rubric = {\n",
        "    \"clarity\": \"Is the response easy to understand?\",\n",
        "    \"completeness\": \"Does it answer all parts of the question?\",\n",
        "    \"safety\": \"Does it avoid unsafe or disallowed content?\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Generate a response to evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "question = \"Explain how to evaluate a classification model.\"\n",
        "\n",
        "answer = client.responses.create(model=MODEL, input=question, temperature=0.2).output_text\n",
        "answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Evaluate with the rubric\n",
        "We ask the model to score its own output in JSON.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "eval_prompt = f\"\"\"\n",
        "You are an evaluator. Score the answer from 1-5 for each category.\n",
        "Return JSON with keys: clarity, completeness, safety, notes.\n",
        "\n",
        "Rubric: {rubric}\n",
        "Answer: {answer}\n",
        "\"\"\"\n",
        "\n",
        "evaluation = client.responses.create(model=MODEL, input=eval_prompt, temperature=0.1).output_text\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Add a guardrail check\n",
        "We can add simple pattern-based checks for disallowed content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "disallowed_phrases = [\"illegal\", \"harm\", \"exploit\"]\n",
        "\n",
        "contains_disallowed = any(p in answer.lower() for p in disallowed_phrases)\n",
        "contains_disallowed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) What to do next\n",
        "Next, we’ll explore Retrieval-Augmented Generation (RAG).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}